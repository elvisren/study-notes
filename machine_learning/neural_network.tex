\chapter{Neural Network}


% definition

\section{Shallow Neural Network}

\subsection{Scalar Input}

In the simplest case, the shallow network is an affine transformation (linear regression) which has linear behavior:
\begin{equation}
    y = b + wx
\end{equation}

Here $b$ is called \cindex{bias}, and $w$ is called \cindex{weight}.

A non-linear activation function $\phi$ is added after the linear regression. ReLU is often the choice. So we have:
\begin{equation}
    y = \phi (b + wx)
\end{equation}

In shallow neural network, it has only one hidden layer with $n$ \cindex{hidden unit}s. So
\begin{equation}
    y = b + \sum_{i=1}^n \phi (b_i + w_i x)
\end{equation}

Each ReLU function will create two piece-wise linear function. So $n$ hidden unit will create a $n+1$ piece-wise linear function. There is an \cindex{universal approximation theorem} that proves for any continuous function, there is a shallow network that can approximate this function to any specified precision.

\subsection{Vector Input}

When the input is a vector, the hidden unit will first aggregate all value and then apply the activation function. If the dimension of vector input is $m$ and the size of hidden input is $n$, the number of linear regions is between $2^m$ and $2^n$.




% activation functions

\section{Activation Functions}

\begin{table}[H]
\centering
\begin{tabular}[t]{lll}
Name & Definition & Range \\ \hline
Sigmoid & $\sigma(a) = \frac{1}{1 + e^{-a}}$ & $(0,1)$ \\
Hyperbolic tangent & $\tanh(a) = 2\sigma(2a) -1$ & $(-1,1)$ \\
Softplus & $\sigma_{+}(a) = \log(1 + e^a)$ & $(0,\infty)$ \\
ReLU & $\text{ReLU}(a) = \max(a,0)$ & $(0,\infty)$ \\
LRelu & $\max(a,0)+\alpha \min(a,0)$ & $(-\infty , \infty)$ \\
ELU & $\max(a,0) + \min(\alpha(e^a - 1), 0)$ & $(-\infty , \infty)$ \\
SELU & $\lambda \text{ELU}$ &$(-\infty , \infty)$ \\
Swish & $a\sigma(a)$ &$(-\infty , \infty)$ \\
GELU & $a \Phi(a)$ &$(-\infty , \infty)$ \\
\end{tabular}
\end{table}

\begin{example}[Relu]
    The problem with ReLU is that if the weights were initialized with large negative value, it will saturate, which is called \cindex{dead ReLU}.
\end{example}

\begin{example}[LReLU]
    Leaky ReLU has a $\alpha$, which could be learned and called parametric ReLU.    
\end{example}

\begin{example}[ELU]
    ELU (exponential linear unit) is better than LReLU for being a smooth function.    
\end{example}


\begin{definition}[SELU]
    SELU (self-normalizing ELU) $\text{SELU}(a;\alpha,\lambda) = \lambda \text{ELU}(a;\alpha)$ is even better that if the input $x$ is normal, each layer keeps previous layer's mean and variance.     
    
    MLP is an iterative function: $x$, $\text{SELU}(x)$, $\text{SELU}(\text{SELU}(x))$, $\dots$. $\text{SELU}$ has a fixed point that $\lambda = 1.0505$ and $\alpha = 1.6732$. 

\end{definition}


\begin{example}[Swish]
    The full definition of Swish function is $a\sigma(\beta a)$. In practice, $\beta$ is always $1$. 
\end{example}

\begin{example}[GELU]
    GELU (gaussian error linear unit) is defined as $a \Phi(a)$ where $\Phi$ is
    \begin{equation}
        \Phi(a) = \mathcal{P}(\mathcal{N}(0,1) \leq a) = \frac{1}{2}\mleft(1+\text{erf}\mleft(\frac{a}{\sqrt{2}}\mright)\mright)
    \end{equation}
    
    $\text{GELU}(a) \approx a \sigma(1.702 a)$. 
\end{example}



% definition
\section{Deep Neural Network}

\defiref{multi_layer_percepton} defined $l$ layers of \cindex{MLP} (\cindex{multi-layer percepton}). Each layer is a hidden unit $z_l$ with differentiable activation function $\varphi$:
\begin{equation}
    z_l = f_l (z_{l-1}) = \varphi (b_l + W_l \times z_{l-1})
\end{equation}

The dimension of the matrix $W_l$ is $z_l \times z_{l-1}$.

There are rules in choosing the activation function $\varphi$:
\begin{enumerate}
    \item It cannot be linear because the whole model would become linear.
    \item The derivative cannot vanish, or saturate. So sigmoid function is not good. ReLU is better, but it still has half plane with zero gradient.
\end{enumerate}

The number of hidden layers is the \cindex{depth} of the network. The number of hidden units for each layer is called \cindex{width} of the network. The total number of hidden units is called the \cindex{capacity} of the network.

\begin{table}[H]
\centering
\begin{tabular}[t]{p{5cm}ccc}
Network & Hidden Units & Regions & Parameters \\ \hline
shallow & $n$ & $n+1$ & $3n+1$\\
deep with one input, one output, and $k$ layers of $n$ hidden units & $k n$ & $(n+1)^k$ & $3n+1+(k-1)n(n+1)$ \\
\end{tabular}
\end{table}


Efficiency of deep network:
\begin{itemize}
    \item \cindex{depth efficiency}: a shallow network may need exponential number of hidden units.
    \item \cindex{width efficiency}: a wide shallow network can be expressed by narrow network with polynomial depth.
\end{itemize}


% loss function

\section{Loss Function}

\subsection{Binary Classification}

In binary classification, $y \in \set{0,1}$. The probability function is
\begin{equation}
    \probability{y|p} = (1-p)^{1-y} \times p^y
\end{equation}


Often the \cindex{logistic regression} is used. Because $P$ is a trained variable, it could exceed the range $[0, 1]$. So it uses the \cindex{sigmoid} function $\sigma(t) = \frac{1}{1 + e^(-t)}$ and assigns probability to $\columnvector{X}$ that $\hat{p} = \sigma(\columnvector{X}^\top \columnvector{\theta} )$. The prediction is:
\begin{equation}
	\hat{y} = \begin{cases}
		0 \text{, if } \hat{p} < 0.5 \\
		1 \text{, if } \hat{p} \geq 0.5
	\end{cases}
\end{equation}

So the prediction is $1$ if $\columnvector{X}^\top \columnvector{\theta} > 0$ and $0$ if it is negative.

The \cindex{log loss} cost function is:
\begin{equation}
	J(\theta) = - \frac{1}{m} \sum_{i=1}^m \mleft\{ \subscription{Y}{i} \log \subscription{\hat{p}}{i} + \mleft(1- \subscription{Y}{i} \mright) \log \mleft(1 - \subscription{\hat{p}}{i} \mright) \mright\}
\end{equation}

The derivative is:
\begin{equation}
	\dpd{J(\theta)}{\theta_j} = \frac{1}{m} \sum_{i=1}^m \mleft( \sigma \mleft(\columnvector{\theta}^\top \subscription{X}{i} \mright) - \subscription{Y}{i} \mright) \subscription{X}{i}_j
\end{equation}






\subsection{Multiclass classification}

In multiclass classification, the prediction is a conditional probability distribution. So we define $f_c$ as
    \begin{equation}
        f_c(x;\theta) = p(y=c|x;\theta)
    \end{equation}
    So $f_C$ is a function $f_C: X \rightarrow [0,1]^C$ and $\displaystyle \sum_{i=1}^C f_i = 1$. This is a strong requirement on $f_C$. We could relax the requirement of $f_C$ by using the \cindex{softmax} function $\mathcal{S}$. Let $\columnvector{a} = f(x;\theta)$ be the logic, we have:
    \begin{equation}
        \mathcal{S}(\columnvector{a}) = \begin{pmatrix}\displaystyle
            \frac{e^{a_1}}{\sum_{i=1}^C e^{a_i}}, ..., \frac{e^{a_C}}{\sum_{i=1}^C e^{a_i}}
        \end{pmatrix}
    \end{equation}
    
    So the probability distribution could be simplified as
    \begin{equation}
        p(y=c|x;\theta) = \mathcal{S}_c \mleft(f(x;\theta)\mright)
    \end{equation}
    
    The loss function in the probability case is 
    \begin{equation}
        L(y, f(x;\theta)) = - \log p(y|f(x;\theta))
    \end{equation}


\cindex{Softmax Regression} is a multiclass version of \cindex{logistic regression}. Each class has its own parameter vector $\columnvector{\theta}_i$. The softmax score for class $k$ is $s_k (\columnvector{X} ) = \columnvector{X}^\top \columnvector{\theta}_k$ and its probability is:
\begin{equation}
    \hat{p}_k = \frac{\exp{{s_k (\columnvector{X})}}}{\displaystyle \sum_{i=1}^n \exp{s_i (\columnvector{X}) } }
\end{equation}

The prediction is:
\begin{equation}
	\hat{y} = \underset{k}{\text{argmax }} \columnvector{X}^\top \columnvector{\theta}_k
\end{equation}

The cost function is measures using \cindex{cross entropy} function:
\begin{equation}
	J(\theta)= -\frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \subscription{Y}{i}_k \log \subscription{\hat{p}}{i}_k
\end{equation}


The gradient vector is:
\begin{equation}
	\nabla_{\theta_k} J(\theta) = \frac{1}{m} \sum_{i=1}^m(\hat{p}_k^{(i)} - y_k^{(i)}) \mathbf{X}^i
\end{equation}


The cross entropy function is also called \cindex{Kullback-Leibler divergence}.


% backprop
\section{Backpropagation}

Backwardpropagation is used in optimizing smooth functions. It is also called \cindex{automatic differentiation} or \cindex{autodiff}. It will construct a computation graph and calculate the gradient of $x_i$ by summing up all child $k$:
\begin{equation}
    \dpd{o}{x_i} = \sum_{k \in \text{children}(i)} \dpd{o}{x_k} \dpd{x_k}{x_i}
\end{equation}

If the MLP is fully connected, each layer is a proper Jacobian matrix:
\begin{equation}
    J_f(x) = J_{f_L} \circ J_{f_{L-1}} \circ \cdots \circ J_{f_1}(x)
\end{equation}

The computation graph could be build ahead of time, which is called \cindex{static graph}. It could be computed just in time by tracing the execution of the function on an input, which is called \cindex{dynamic graph}.





% fitting model
\section{Fitting the Model}

\subsection{Gradient Descent}
The classic way is to use \cindex{gradient descent}:
\begin{equation}
    \phi \leftarrow \phi - \alpha \dpd{L}{\phi}
\end{equation}

$\alpha$ is called the \cindex{learning rate}. It works well for convex model, but not good for non-convex ones. 


\subsection{Stochastic Gradient Descent}
The second choice is to use \cindex{stochastic gradient descent}. It trains the model using subset of training data called \cindex{batch} or \cindex{minibatch} \cindex{without replacement}. A single pass through all training data is called \cindex{epoch}. It is usually accompanied with \cindex{learning rate schedule}. $\alpha$ will start high and decrease by a constant factor every few epochs.

\subsection{Momentum}

The next one is to use \cindex{momentum}. The momentum term tries to predict the next gradient:

\begin{equation}
    \begin{aligned}
        m_{t+1} &\leftarrow \beta \cdot m_t + (1-\beta) \sum_{i \in B_t} \dpd{L_t(\phi_t)}{\phi} \\
        \phi_{t+1} &\leftarrow \phi_t - {\color{red}\alpha} \cdot m_{t+1}
    \end{aligned}
\end{equation}

It will move in two steps:
\begin{enumerate}
    \item move according to the current gradient $\displaystyle \sum_{i \in B_t} \dpd{L_t(\phi_t)}{\phi}$
    \item move according to the previous gradient $m_t$
\end{enumerate}


\subsection{Nesterov Accelerated Momentum}

In \cindex{Nesterov acclerated momentum}, it tries to predict the gradient at predicted point rather than at current point:
\begin{equation}
    \begin{aligned}
        m_{t+1} &\leftarrow \beta \cdot m_t + (1-\beta)\sum_{i \in B_t} \dpd{L_t({\color{red} \phi_t - \alpha \beta m_t})}{\phi} \\
        \phi_{t+1} &\leftarrow \phi_t - \alpha \cdot m_{t+1}
    \end{aligned}
\end{equation}

It will move in two steps:
\begin{enumerate}
    \item move according to the previous gradient $m_t$
    \item move according to current gradient $\displaystyle \sum_{i \in B_t} \dpd{L_t(\phi_t - \alpha \beta m_t)}{\phi}$
\end{enumerate}


\subsection{Adam}

\cindex{adaptive moment estimation}(\cindex{Adam}) moves fixed size every time. For $\beta, \gamma \in [0,1)$, we have
\begin{equation}
    \begin{aligned}
        m_{t+1} &\leftarrow \beta \cdot m_t + (1-\beta) \sum_{i \in B_t} \dpd{L_i (\phi_i)}{\phi} \\
        v_{t+1} &\leftarrow \gamma \cdot v_t + (1-\gamma) \sum_{i \in B_t} \mleft({\dpd{L_i (\phi_i)}{\phi}}\mright)^2 \\
        \tilde{m}_{t+1} &\leftarrow \frac{m_{t+1}}{1- \beta^{t+1}} \\
        \tilde{v}_{t+1} &\leftarrow \frac{v_{t+1}}{1- \gamma^{t+1}} \\
        \phi_{t+1} &\leftarrow \phi_t - \alpha \cdot \frac{\tilde{m}_{t+1}}{\sqrt{\tilde{v}_{t+1}} + \epsilon}
    \end{aligned}
\end{equation}


% Initialization
\section{Initialization}



% measure performance
\section{Measure Performance}

% regularization
\section{Regularization}


% regression
\section{Regression}

MLP could be used for regression. It will train two functions $f_\mu (x)$ for the mean and $f_\sigma (x)$ for the standard derivation. $f_\mu(x)$ and $f_\sigma(x)$ could share most of the layers called backbone, and with two output (heads):
\begin{enumerate}
    \item For the $\mu$ head, use linear activator $\varphi(a) = a$
    \item For the $\sigma$ head, use softplus activator $\varphi(a) = \log(1 + e^a)$
\end{enumerate}

So the result is a learned normal distribution
\begin{equation}
    p(y|x,\theta) = \mathcal{N}\mleft(y| \transpose{w_\mu} f(x;w_{\text{shared}}), \sigma_{+} \mleft( \transpose{w_\mu} f(x;w_{\text{shared}}) \mright)  \mright)
\end{equation}
















































































































































































































































