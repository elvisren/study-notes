\chapter{Neural Network}


% definition
\section{Definition}

\defiref{multi_layer_percepton} defined $l$ layers of MLP (multi-layer percepton). Each layer is a hidden unit $z_l$ with differentiable activation function $\varphi$:
\begin{equation}
    z_l = f_l (z_{l-1}) = \varphi (b_l + W_l \times z_{l-1})
\end{equation}


There are rules in choosing the activation function $\varphi$:
\begin{enumerate}
    \item It cannot be linear because the whole model would become linear
    \item The derivative cannot vanish, or saturate. So sigmoid function is not good. ReLU is better, but it still has half plane with zero gradient
\end{enumerate}



% regression
\section{Regression}

MLP could be used for regression. It will train two functions $f_\mu (x)$ for the mean and $f_\sigma (x)$ for the standard derivation. $f_\mu(x)$ and $f_\sigma(x)$ could share most of the layers called backbone, and with two output (heads):
\begin{enumerate}
    \item For the $\mu$ head, use linear activator $\varphi(a) = a$
    \item For the $\sigma$ head, use softplus activator $\varphi(a) = \log(1 + e^a)$
\end{enumerate}

So the result is a learned normal distribution
\begin{equation}
    p(y|x,\theta) = \mathcal{N}\left(y| \transpose{w_\mu} f(x;w_{\text{shared}}), \sigma_{+} \left( \transpose{w_\mu} f(x;w_{\text{shared}}) \right)  \right)
\end{equation}


% backprop
\section{Backpropagation}

Backpropagation is also called automatic differentiation or autodiff. It will construct a computation graph and calculate the gradient of $x_i$ by summing up all child $k$:
\begin{equation}
    \dpd{o}{x_i} = \sum_{k \in \text{children}(i)} \dpd{o}{x_k} \dpd{x_k}{x_i}
\end{equation}

If the MLP is fully connected, each layer is a proper Jacobian matrix:
\begin{equation}
    J_f(x) = J_{f_L} \circ J_{f_{L-1}} \circ \cdots \circ J_{f_1}(x)
\end{equation}

The computation graph could be build ahead of time, which is called static graph. It could be computed just in time by tracing the execution of the function on an input, which is called dynamic graph.


















































































































































































































































