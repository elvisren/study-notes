\chapter{Linear Model}


% definition
\section{Definitions}
\begin{definition}
    In supervised learning, we have 
    \begin{enumerate}
        \item The task $T$ is to learn a mapping $f: X \rightarrow Y$.
        \item $X$ is called \cindex{feature}.
        \item $Y$ is called \cindex{labels}.
        \item $X = \realnumber^D$ and $D$ is the dimension of \cindex{feature vector}.
        \item The experience $E$ is $\mathcal{D} = \set{\left(\selectonesample{x}{i}, \selectonesample{y}{i}\right)}_{i=1}^N$, which is called the \cindex{training set}.
        \item $N$ is the \cindex{sample size}.
        \item The input feature is a $N \times D$ matrix, which is called \cindex{design matrix}. In practice the design matrix is usually transposed with dimension $D \times N$.
    \end{enumerate}
\end{definition}


% simplest case
\section{Simplest Case}

In linear regression, $\feature{x}$ is the input variable with $D$ dimension. The data set has $N$ such input variables. In the simplest case, the linear regression is
\begin{align}
    y = f(\feature{x}, \feature{w}) = w_0 + w_1 \selectonedimension{x}{1} + w_2 \selectonedimension{x}{2} + \cdots + w_D \selectonedimension{x}{D}
\end{align}

$w_0$ is usually written as $b$ which is the \cindex{bias}. 


% basis function
\section{Basis Function}

One enhancement is to add non-linearity to the linear regression. Here we choose $M$ non-linear function $\set{\phi_j}$ called \cindex{basis function}:
\begin{equation}
    y = f(\feature{x}, \feature{w}) = \sum_{j=1}^M w_j \phi_j (\feature{x}) = \transpose{\feature{w}} \phi(\feature{x})
\end{equation}

If $M = N+1$, $\phi_1(\feature{x}) = w_0 = b$, $\phi_j(\feature{x}) = \selectonedimension{x}{j}$, the result is the simplest linear regression.

$\phi(\feature{x})$ has other choices.



% maximum likelihood

\section{Maximum Likelihood}

Now we assume the target variable $y$ is the sum of deterministic function $f(\feature{x}, \feature{w})$ with additional normal noise $\epsilon$. So the result $y$ is a normal distribution with mean $f(\feature{x}, \feature{w})$ and variance $\sigma^2$. So
\begin{equation}
    y \sim p(y|\feature{x}, \feature{w}, \sigma^2) = \normaldistributionwithparameter{y}{f(\feature{x}, \feature{w})}{\sigma^2}
\end{equation}

For $N$ input sample $\set{\selectonesample{x}{i}}$ and $N$ target $\set{\selectonesample{y}{i}}$, the likelihood function is
\begin{equation}
    p(y|\feature{x}, \feature{w}, \sigma^2) = \prod_{i=1}^N \normaldistributionwithparameter{\selectonesample{y}{i}}{\transpose{\feature{w}} \phi\left(\selectonesample{x}{i}\right)}{\sigma^2}
\end{equation}

Use homoscedastic regression formula in \formularef{homoscedastic_regression}, we have 
\begin{equation}\label{loss_function_of_linear_regression}
    L(\feature{w}) = N \ln \sigma + \frac{1}{2\sigma^2} \sum_{i=1}^N \left( \selectonesample{y}{i} - \transpose{\feature{w}} \phi\left(\selectonesample{x}{i}\right) \right)^2
\end{equation}

To find the closed solution, we define $\Psi_{N \times M}$ as $\Psi_{ij} = \phi_j \left(\selectonesample{x}{i}\right)$. So each row of $\Psi$ is the result of $\selectonesample{x}{i}$ and apply all $\phi$. Then we define the pseudo inverse of $\Psi$ as $\pseudoinverse{\Psi} = \inverse{\left(\transpose{\Psi} \Psi\right)} \transpose{\Psi}$. The closed form solution is
\begin{equation}\label{closed_simple_linear_regression_solution}
    \begin{aligned}
        \Psi_{ij} &= \phi_j \left(\selectonesample{x}{i}\right) \\
        \pseudoinverse{\Psi} &= \inverse{\left(\transpose{\Psi} \Psi\right)} \transpose{\Psi} \\
        \estimation{\feature{w}} &= \pseudoinverse{\Psi} Y \\
        \estimation{\sigma^2} &= \frac{1}{N} \sum_{i=1}^N \left( \selectonesample{y}{i} - \transpose{\estimation{\feature{w}}} \phi\left( \selectonesample{x}{i} \right) \right)^2
    \end{aligned}
\end{equation}


In practice, it will have numeric difficulty in calculating $\inverse{\left(\transpose{\Psi} \Psi\right)}$ when the result is close to singular. In this case, SVD is used instead.


% online algorithm
\section{Online Algorithm}

\formularef{closed_simple_linear_regression_solution} is called \cindex{batch mode} because it runs over the entire training set. If the data is sufficiently large, it may be worthwhile to use sequential algorithm, which is called \cindex{online algorithms}.

Stochastic gradient descent is used here. The gradient of $L(w)$ is
\begin{equation}
    \nabla L(w) = - \left( \selectonesample{y}{i} - \transpose{\feature{w}} \phi\left(\selectonesample{x}{i}\right) \right) \phi\left(\selectonesample{x}{i}\right)
\end{equation}

So the gradient descent result is
\begin{equation}
    \feature{w}_{t+1} = \feature{w}_t + \alpha \left( \selectonesample{y}{t} - \transpose{\feature{w_t}} \phi\left(\selectonesample{x}{t}\right) \right) \phi\left(\selectonesample{x}{t}\right)
\end{equation}


% regularization
\section{Regularization}

One way of controlling over-fitting is to add regularization. The simplest one is to add $\frac{1}{2} \transpose{\feature{w}}\feature{w}$. So the optimization target is now
\begin{equation}
    \argmin{\feature{w}} \frac{1}{2} \sum_{i=1}^N \left( \selectonesample{y}{i} - \transpose{\feature{w}} \phi\left(\selectonesample{x}{i}\right) \right)^2 + \frac{\lambda}{2} \transpose{\feature{w}}\feature{w}
\end{equation}

It has a closed solution
\begin{equation}
    \estimation{\feature{w}} = \inverse{\left(\lambda I + \transpose{\Psi} \Psi\right)} \transpose{\Psi} 
\end{equation}



% decision theory
\section{Decision Theory}

Suppose we have two random variable $x$ and $y$, and we use $f(x)$ to predict $y$. There is also a loss function $L(y, f(x))$ that measure the cost. The average loss is calculated as
\begin{equation}
    \expectation{L} = \iint L\left(y, f(x)\right) p(x,y) \dif x \dif y
\end{equation}

If we choose the loss as
\begin{equation}\label{square_loss}
    L(x, y) = (x-y)^2
\end{equation}

The average loss is now
\begin{equation}
    \expectation{L} = \iint \left(y - f(x)\right)^2 p(x,y) \dif x \dif y
\end{equation}

Take the derivative and the closed form solution is
\begin{equation}
    2 \int \left( f(x) -y \right) p(x,y) \dif y = 0 \Rightarrow f(x) = \frac{\int y p(x,y) \dif y}{p(x)} = \expectation{y|x}
\end{equation}

Other loss function could be used too, such as \cindex{Minkowski loss} $L(x,y) = (x - y)^p$.


% bias-variance tradeoff
\section{Bias-variance Tradeoff}

The loss function defined in \formularef{square_loss} could be re-arranged as
\begin{equation}
    \begin{aligned}
        (f(x) - y)^2 &= (f(x) - \expectation{y|x} + \expectation{y|x} - y)^2 \\
        &= (f(x) - \expectation{y|x})^2 + 2 (f(x) - \expectation{y|x})(\expectation{y|x} - y) + (\expectation{y|x} - y)^2
    \end{aligned}
\end{equation}

Take expectation on both side, and we have
\begin{equation}
    \expectation{L} = \underbrace{\int (f(x) - \expectation{y|x})^2 p(x) \dif x}_{\text{first part}} + \underbrace{\int \variance{y|x} p(x) \dif x}_{\text{second part: noise}}
\end{equation}

The second part is unrelated to the $f(x)$, and it is the intrinsic variability of the target data and can be regarded as noise. 

In practice, $f(x)$ also depends on the training set $D$, so it is of the form $f(x;D)$ with $D$ being probabilistic. So we have the fomula
\begin{equation}
    \begin{aligned}
        (f(x;D) - \expectation{y|x})^2 &= (f(x;D) - \expectationonanvariable{D}{f(x;D)} + \expectationonanvariable{D}{f(x;D)} - \expectation{y|x})^2 \\
        &= (f(x;D) - \expectationonanvariable{D}{f(x;D)})^2 \\
        &+ (\expectationonanvariable{D}{f(x;D)} - \expectation{y|x})^2 \\
        &+ 2 (f(x;D) - \expectationonanvariable{D}{f(x;D)})(\expectationonanvariable{D}{f(x;D)} - \expectation{y|x})
    \end{aligned}
\end{equation}

Take the expectation, and we have
\begin{equation}
    \underbrace{\int (f(x) - \expectation{y|x})^2 p(x) \dif x}_{\text{first part}} = \underbrace{(\expectationonanvariable{D}{f(x;D)} - \expectation{y|x})^2}_{\text{bias}^2} + \underbrace{\expectationonanvariable{D}{(f(x;D) - \expectationonanvariable{D}{f(x;D)})^2}}_{\text{variance}}
\end{equation}

So 
\begin{equation}
    \text{expected loss} = \text{bias}^2 + \text{variance} + \text{noise}
\end{equation}
where
\begin{equation}
    \begin{aligned}
        \text{bias}^2 &= \left(\expectationonanvariable{D}{f(x;D)} - \expectation{y|x}\right)^2 \\
        \text{variance} &= \expectationonanvariable{D}{\left(f(x;D) - \expectationonanvariable{D}{f(x;D)}\right)^2} \\
        \text{noise} &= \int \variance{y|x} p(x) \dif x
    \end{aligned}
\end{equation}

The explanation of bias, variance, and noise are:
\begin{itemize}
    \item Bias measure the average prediction diff between prediction and desired result
    \item Variance measure variance of the prediction itself
    \item Noise is the intrinsic variability of the target data
\end{itemize}

In practice, the bias-variance decomposition is of limited use.































































































































































































































































































































































































































































































































































































































































































































































