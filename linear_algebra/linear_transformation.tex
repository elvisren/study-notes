
% linear transformation
\chapter{Linear Transformation}

The logic of this section:
\begin{enumerate}
    \item Define linear transformation
    \item Introduce linear operator
    \item Define the coordinate of vector
    \item Define the coordinate matrix of linear transformation
    \item Prove linear transformation is a vector space with additional multiplication operation
    \item Introduce matrix multiplication
    \item Define inverse of square matrix
    \item Define change-of-coordinate matrix
\end{enumerate}

\section{Definition}

\begin{definition}[\cindex{linear transformation}]
    A function $T:V \rightarrow W$ is a linear transformation if 
    \begin{equation}
        T(cx + y) = cT(x) + T(y)
    \end{equation}
\end{definition}

There are few basic terms for linear transformation:
\begin{itemize}
    \item $I_V$: identity transformation that $I_V (x) = x$
    \item $I_0$: zero transformation that $I_0 (x) = 0$
\end{itemize}



\begin{definition}[\cindex{linear operator}]
    If $V = W$, $T$ is called linear operator. Linear operator is the focus of inner product.
\end{definition}


\begin{definition}[\cindex{rank}]\label{rank-of-linear-transformation}
    $\nullspace{T}$ and $\rangespace{T}$ are all spaces. $\dimension{\rangespace{T}}$ is the rank\footnote{Here the rank of $T$ is the dimension of range space. In \theoref{rank-of-matrix} we define the rank of a matrix $A$ which is the linearly independent rows or columns of $A$. So when we define $A = \coordinate{T}_\beta$, both ranks are the same.} of $T$.
\end{definition}



\begin{theorem}
    \begin{equation}
        \text{nullity }(T) + \rank{T} = \dimension{V}
    \end{equation}
\end{theorem}


If (and only if) $\nullspace{T} = \set{0}$, $T$ is a one-to-one mapping.


\begin{theorem}[\cindex{$\mathcal {L}(V,W)$}]
    All linear transformation is denoted by $\mathcal {L}(V,W)$. It is a linear space.
\end{theorem}


As the behavior of a vector space is completely defined by its basis, and the behavior of linear transformation is also determined by its action on basis, as the following theorem shows:
\begin{theorem}    
    Let $\set{v_i}$ be $n$ basis of $V$ and $\set{w_j}$ be any $n$ vectors from $W$ ($\set{w_j}$ do not need to be independent). There is a unique $T \in \mathcal{L}(V,W)$ that
    \begin{equation}
        T v_k = w_k
    \end{equation}
\end{theorem}






\section{Coordinates}

We define the coordinate of a vector and a linear transformation. Let $\beta = \set{u_i}$ be an \cindex{ordered basis}. If $\displaystyle x = \sum_{i=1}^n a_i u_i$, we define the coordinate of $x$ relative to $\beta$ as :
\begin{equation}
    x = \sum_{i=1}^n a_i u_i \Longrightarrow \coordinate{x}_\beta = \begin{pmatrix}
        a_1 \\
        a_2 \\
        \vdots \\
        a_n
    \end{pmatrix}
\end{equation}

For linear transformation $T$ from $\beta = \set{v_i} $ to $\gamma = \set{w_j}$, we have :
\begin{equation}
    T(v_j) = \sum_{i=1}^m a_{ij} w_i \Longrightarrow \coordinate{T}_{\beta}^{\gamma} = \begin{pmatrix}
        \hdots & a_{1j} & \hdots \\
        \hdots & a_{2j} & \hdots \\
        & \vdots \\
        \hdots & a_{nj} & \hdots \\
    \end{pmatrix}
\end{equation}

Many important result in later chapters will be the choice of basis $\beta$ that makes $\coordinate{T}_\beta$ as simple as possible.

% matrix multiplication
\section{Matrix Multiplication}

\begin{theorem}
    Linear transformation is linear in $+$ and $\times$ operations. So we could define the product of two linear transformations $UT$. It turns out to be the standard matrix multiplication. And we have

    \begin{equation}\label{matrix_coordinate}
        \coordinate{UT}_{\alpha}^{\gamma} = \coordinate{U}_{\beta}^{\gamma} \times \coordinate{T}_{\alpha}^{\beta}
    \end{equation}
\end{theorem}

\begin{theorem}
    Let $\selectmatrixcolumn{A}{j}$ be the $j$-th column of $A$, we have
\begin{enumerate}
    \item $\selectmatrixcolumn{(AB)}{j} = A \times \selectmatrixcolumn{B}{j}$
    \item $\selectmatrixcolumn{A}{j} = A \times \selectmatrixcolumn{I}{j} = A \times e_j$
\end{enumerate}
\end{theorem}





\begin{theorem}
    For $T: V \rightarrow W$,
    \begin{equation}
        \coordinate{T(u)}_\gamma = \coordinate{T}_{\beta}^{\gamma} \times \coordinate{u}_{\beta}
    \end{equation}    
\end{theorem}
\begin{proof}
    The idea is to find a linear transformation of $u$. Fix a $u$ and define a function $f: F \rightarrow V$ as $f(a) = au$, and $\set{1}$ is the standard basis for $F$. So $u = \coordinate{f}_{\set{1}}$. Let $g=Tf$ and use Formula~\ref{matrix_coordinate} to prove it: $\coordinate{T(u)}_\gamma = \coordinate{g(1)}_\gamma = \coordinate{g}_{\alpha}^{\gamma}$.
\end{proof}

From any matrix $A$, we could define a left multiplication transformation $L_A$.


\begin{example}
    One property of trace is that for $A_{m \times n}$ and $B_{n \times m}$, we have
\begin{equation}
    \trace{AB} = \trace{BA}
\end{equation}
\end{example}


\begin{theorem}\label{column-row-factorization}
    Suppose the column rank of $A_{m \times n}$ is $c \leq 0$. Then there exists $L_{m \times c}$ and $R_{c \times n}$ that
    \begin{equation}
        A_{m \times n} = L_{m \times c} \times R_{c \times n}
    \end{equation}
\end{theorem}
\begin{proof}
    Since the column rank of $A$ is $c$, all $A$'s columns $\selectmatrixcolumn{A}{i}$ could be reduced into a basis of $c$ vectors. Put them together to form $L$. Then for each of $\selectmatrixcolumn{A}{i}$, $\selectmatrixcolumn{D}{j} = \coordinate{\selectmatrixcolumn{A}{j}}_L$.
\end{proof}

\begin{theorem}[\cindex{rank}]\label{rank-of-matrix}
    Suppose $A \in F^{m,n}$. Then the column rank of $A$ is the same as the row rank of $A$. We will define it as the rank\footnote{In \defiref{rank-of-linear-transformation} we defined the rank of linear transformation. These two ranks are the same if $A=\coordinate{T}_\beta$.} of $A$.
\end{theorem}
\begin{proof}
    Use \theoref{column-row-factorization} to decompose $A$ into $A = LR$. Since $\selectmatrixrow{A}{i}$ is a linear combination of rows of $R$, the row rank of $A$ is less than $c$, which is the column rank of $A$. So
    \begin{equation*}
        \text{row rank of } A \leq \text{column rank of } A
    \end{equation*}
    
    Because $\text{column rank of } A = \text{row rank of } \transpose{A} \leq \text{column rank of } \transpose{A} = \text{row rank of } A$, we have $\text{row rank of } A = \text{column rank of } A$.
\end{proof}



%
%
% inverse
%
%


\section{Inverse}
\begin{definition}[\cindex{inverse}]
    The inverse $\inverse{T}$ of a linear transformation $T:V \rightarrow W$ requires that both $T \inverse{T} = I_W$ and $\inverse{T} T = I_V$. It requires that $\dimension{V} = \dimension{W}$. We have
\begin{equation}
    \coordinate{\inverse{T}}_{\gamma}^{\beta} = \inverse{\mleft( \coordinate{T}_{\beta}^{\gamma} \mright)}
\end{equation}

Non-invertible is also called \cindex{singular}.
\end{definition}



\begin{definition}[\cindex{isomorphic}]
	$V$ is isomorphic to $W$ if there exists a linear transformation $T:V\rightarrow W$ that is invertible. $T$ is called an \cindex{isomorphism}  from $V$ to $W$.
\end{definition}


\begin{theorem}
	The function $\Phi: \mathcal{L}(V,M) \rightarrow M_{m \times n}(F)$ defined by $\Phi (T) = \coordinate{T}_\beta^\gamma$, is an isomorphism. The dimensions have a relation that 
	\begin{equation}
		\dimension{\mathcal{L}(V,M)} = \dimension{V} \times \dimension{W}
	\end{equation}
\end{theorem}

%
%
% change of coordinates
%
%

\section{Change of Coordinates}

If we change the basis of a vector space, we may change the coordinate of vector and linear operatos.

We now examine the coordinate relations given two basis. First we see how it impacts vector representation. 
\begin{theorem}[\cindex{change of coordinate}]
    Let $\beta$ and $\beta^{\prime}$ be two basis of $V$, and $Q=\coordinate{I_V}_{\beta^\prime}^\beta$ , we have 
\begin{equation}
    \coordinate{v}_\beta = Q \coordinate{v}_{\beta^\prime}
\end{equation}    

Here $Q$ is the change of coordinate matrix that change from $\beta^\prime$ coordinates into $\beta$ coordinates. $Q$ is invertible and $\inverse{Q}$ will change from $\beta$ coordinates into $\beta^\prime$ coordinates. 

Then we see how it will impact the matrix representation of linear operators:
\begin{equation}\label{change_of_coordinate_formula}
    \coordinate{T}_{\beta^\prime} = \inverse{Q} \coordinate{T}_{\beta} Q = \coordinate{I_V}_\beta^{\beta^\prime} \coordinate{T}_\beta \coordinate{I_V}_{\beta^\prime}^\beta
\end{equation}
\end{theorem}


We also have change of coordinate matrix that represent a general linear transformation, rather than linear operator:

\begin{theorem} \label{specialchangeofcoordinates}
	Let $T:V\rightarrow W$, $\beta$ and $\beta^\prime$ are ordered basis of $V$, $\gamma$ and $\gamma^\prime$ are ordered basis of $W$. Then
	\begin{equation}
		\coordinate{T}_{\beta^\prime}^{\gamma^\prime} = \coordinate{I_W}_\gamma^{\gamma^\prime} \coordinate{T}_\beta^\gamma \coordinate{I_V}_{\beta^\prime}^\beta
	\end{equation}
\end{theorem}


\begin{example}
    For a linear operator $L_A$ on $F^n$, let's see what is the result if we change from stand basis of $F^n$ to non-stand basis $\gamma = \set{v_i}$. The $Q$ now becomes
    \begin{equation*}
        \selectmatrixcolumn{Q}{j} = v_j = [\gamma]
    \end{equation*}
    So $Q = [v_1, v_2, \cdots, v_n]$. Therefore
    \begin{equation}
        [L_A]_\gamma = \inverse{[\gamma]} A [\gamma]
    \end{equation}
\end{example}




\begin{definition}[\cindex{similar}]
    Matrix $A$ and $B$ are similar if there is a $Q$ that $A = \inverse{Q} B Q$.    
\end{definition}


%
%
% example
%
%

\section{Linear Transformation Example}


\begin{example}
    The formula of the reflection about the line $y=tx$. It is a linear transformation $T$ with coordinates under standard basis $\beta$:
    \begin{equation*}
        \coordinate{T}_\beta = \frac{1}{1 + t^2} \begin{pmatrix}
            1-t^2 & 2t \\
            2t & t^2 -1
        \end{pmatrix}
    \end{equation*}
\end{example}


\begin{example}
    Let's try to find the solution to homogeneous differential equation 
    \begin{equation*}
        \sum_{i=0}^n a_i y^{(i)} = 0
    \end{equation*}
    
    Define differential operation $D$ as $D(y) = y^\prime$. $D$ turns out to be a linear operation. With a polynomial $p(t) = \sum_{i=0}^n a_i t^i$, the homogeneous equation becomes $p(D)y = 0$. So we are looking for $\nullspace{p(D)}$.
    
    The solution is a subspace, so we start from small. For $y^\prime + a_0 y = 0$, the solution has dimension 1 with basis $\set{e^{-a_0 t}}$.
    
    For advances case, we could prove that the result is of dimension $n$, so if we could find $n$ linearly independent result, it is the answer. So if $p(x) = 0$ has $n$ distinct solution $\set{c_i}$, the solution to $p(D)y = 0$ is $\set{e^{c_i t}}$. If one solution $m$ appears $k$ times, then $\set{e^{mt}, t e^{mt}, ..., t^{m-1} e^{mt}}$ are part of the solution. 
    
    So if 
    \begin{equation*}
        p(t) = (t-c_1)^{n_1} (t-c_2)^{n_2} \hdots (t-c_k)^{n_k} 
    \end{equation*}
    the basis for the solution is 
    \begin{equation*}
        \set{e^{c_1 t}, t e^{c_1 t}, ... , t^{n_1 -1} e^{c_1 t}, ... ,  e^{c_k t}, t e^{c_k t}, ..., t^{n_k -1} e^{c_k t}}
    \end{equation*}
\end{example}


%
%
% Quotient space
%
%
\section{Quotient Space}

\begin{definition}[\cindex{affine subset}]
    An affine subset of $V$ is a subset of the form $v + U$ for some $v\in V$ and $U \subseteq V$.
    
    The affine subset $v+U$ is said to be parallel to $U$.
\end{definition}

\begin{definition}[\cindex{quotient space}]
    Suppose $U$ is a subspace of $V$. Define quotient space $V/U$ to be the set of all affine sets of $V$ parallel to U:
    \begin{equation}
        V/U = \set{v + U: v \in V}
    \end{equation}
\end{definition}

\begin{definition}[\cindex{quotient map}]
    The quotient map $\pi : V \rightarrow V/U$ is defined by 
    \begin{equation}
        \pi(v) = v + U
    \end{equation}
\end{definition}

\begin{theorem}
    \begin{equation}
        \dimension{V/U} = \dimension{V} - \dimension{U}
    \end{equation}    
\end{theorem}

\begin{theorem}
    Define $\tilde{T}$ as 
    \begin{equation}
        \tilde{T}\mleft(v + \nullspace{T}\mright) = T(v)
    \end{equation}
    
    Then $\tilde{T}$ is an injective linear map.
\end{theorem}




%
%
% dual space
%
%

\section{Dual Space}

\begin{definition}[\cindex{linear functional}]
	A linear functional is a linear transformation that map from $V$ into $F$. So it is an element of $\mathcal{L}(V, F)$.
\end{definition}


\begin{definition}[\cindex{dual space}]
	The dual space\label{dualspacedefinition} of $V$ is the vector space $V^* = \mathcal{L}(V,F)$. The \cindex{double dual space} $V^{**}$ is the dual space of $V^*$.
\end{definition}


The dimension of dual space is $\dimension{V^*} = \dimension{\mathcal{L}(V,F)} = \dimension{V} \times \dimension{F} = \dimension{V}$.


\begin{definition}
	An $i$-th coordinate function $f_i$ with respect to basis $\beta$ is defined as $f_i(x) = a_i$ where
	\begin{equation*}
		\coordinate{x}_\beta = \begin{bmatrix}
			a_1 \\
			a_2 \\
			\vdots \\
			a_n
		\end{bmatrix} = \begin{bmatrix}
			f_1(a) \\
			f_2(a) \\
			\vdots \\
			f_n(a)
		\end{bmatrix}
	\end{equation*}
\end{definition}




\begin{definition}[\cindex{dual basis}]
	Let $\beta = \set{x_i}$ be an ordered basis for finite dimensional vector space $V$. Let $f_i$ be the $i$-th coordinate function with respect to basis $\beta$. let $\beta^*=\set{f_i}$. Then $\beta^*$ is an ordered basis for $V^*$, and $\forall f \in V^*$, we have
	\begin{equation}
		f = \sum_{i=1}^n f(x_i) f_i
	\end{equation}
	$\beta^*$ is called the dual basis of $\beta$.
\end{definition}
\begin{proof}
	Let $g =\displaystyle \sum_{i=1}^n f(x_i) f_i$, we have
	\begin{equation*}
	g(x_j) = \mleft( \sum_{i=1}^n f(x_i) f_i \mright) (x_j) = \sum_{i=1}^n f(x_i) f_i (x_j) = \sum_{i=1}^n f(x_i) \delta_{ij} =f(x_j)
	\end{equation*}
\end{proof}


\begin{theorem}[\cindex{dual map}]
	Let $V$ and $W$ be vector space over $F$ with ordered basis $\beta$ and $\gamma$. For any linear transformation $T:V \rightarrow W$, the mapping $T^t: W^* \rightarrow V^*$ defined as 
	
	\begin{equation}
	    T^\top (g) = g \circ T
	\end{equation}
	
	$T^t$ is called dual map and it	is a linear transformation.	
\end{theorem}
\begin{proof}
	Let $\beta = \{x_i\}$ and $\gamma=\{y_i\}$ with dual basis $\beta^*=\{f_i\}$ and $\gamma^*=\{g_i\}$, $A=\coordinate{T}_\beta^\gamma$. we have
	\begin{equation*}
		T^\top (g_j) = g_j T = \sum_{s=1}^n (g_j T) (x_s) f_s
	\end{equation*}
	
	So the row $i$, column $j$ entry of $[T^\top]_{\gamma^*}^{\beta^*}$ is
	\begin{equation*}
	(g_j T)(x_i) = g_j (T(x_i))= g_j \mleft( \sum_{k=1}^m A_{kj} y_k \mright) = \sum_{k=1}^m A_{kj} g_j(y_k)= \sum_{k=1}^m A_{kj} \delta_{kj} = A_{ji}
	\end{equation*}
	
	Hence $\mleft[T^\top \mright]_{\gamma^*}^{\beta^*} = A^\top $.
\end{proof}

\begin{definition}
    For $U \subset V$, the \cindex{annihilator} of $U$, denoted as $U^0_V$, is defined as
    \begin{equation*}
        U^0_V = \set{\phi \in V^*: \phi(u) = 0, \forall u \in U}
    \end{equation*}
    So the annihilator map $U$ to $0$. For vectors in $V - U$, the mapping could be any result. \emph{The annihilator is a subspace}.
\end{definition}

\begin{theorem}
    \begin{equation}
        \dimension{U} + \dimension{U_V^0} = \dimension{V}
    \end{equation}
\end{theorem}

\begin{proof}
    Define $i \in \mathcal{L}(U,V)$ that $i(u) = u, \forall u \in U$. $i^* \in \mathcal{L}(V^*,U^*)$. So
    \begin{equation*}
        \dimension{\rangespace{i^*} } +\dimension{\nullspace{i^*}} = \dimension{V^*}
    \end{equation*}
    By definition, $\nullspace{i^*} = U^0_V$. Also $\rangespace{i^*} = U^*$.
\end{proof}


\begin{theorem}
    Let $V$ and $W$ be two finite-dimentional vector space, and $T \in \mathcal{L}(V,W)$. Then:
    \begin{enumerate}
        \item $\nullspace{T^*}  = (\rangespace{T} )^0$
        \item $\rangespace{T^*} = (\nullspace{T} )^0$
        \item $\dimension{\rangespace{T^*}} = \dimension{\text{range } T}$
        \item $\dimension{\nullspace{T^*}} = \dimension{\nullspace{T}} + \dimension{W} - \dimension{V}$
    \end{enumerate}
\end{theorem}

\begin{proof}
    Suppose $\varphi \in \text{null } T^*$. Then $ 0 = T^*(\varphi) = \varphi T$. Then
    \begin{equation*}
        0 = (\varphi T)(v) = \varphi (Tv) 
    \end{equation*}
    So $\varphi \in (\text{range } T)^0_W$.
    
    \begin{equation*}
        \begin{aligned}
            \dimension{\rangespace{T^*}} &= \dimension{W^*} - \dimension{\nullspace{T^*}} \\
            &= \dimension{W} - \dimension{\rangespace{T}^0} \\
            &= \dimension{\rangespace{T}}
        \end{aligned}
    \end{equation*}
    
    \begin{equation*}
        \begin{aligned}
            \dimension{\nullspace{T^*}} &= \dimension{\rangespace{T}^0}\\
            &= \dimension{W} - \dimension{\rangespace{T}} \\
            &= \dimension{W} - (\dimension{V} - \dimension{\nullspace{T}} ) \\
            &= \dimension{W} + \dimension{\nullspace{T}} - \dimension{V}
        \end{aligned}
    \end{equation*}
\end{proof}

\begin{theorem}
    Let $V$ and $W$ be two finite-dimentional vector space, and $T \in \mathcal{L}(V,W)$. Then
    \begin{equation}
	    \mleft[T^\top \mright]_{\gamma^*}^{\beta^*} = \mleft(\mleft[T \mright]_\beta^\gamma \mright)^\top
	\end{equation}
\end{theorem}




