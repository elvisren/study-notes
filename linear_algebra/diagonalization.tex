\chapter{Diagonalization}

\section{Why it is useful}


The reason we do research on diagonalization is to simplify calculation. If a square matrix is diagonalizable, the result is a diagonal matrix, which is each to multiply.

Diagonalization process will find basis with eigenvectors. These eigenvectors are invariant subspace. So we could use simple logic to analyze the behavior of linear transformation.

\emph{Linear operator has rich property because it is closely linked to polynomial}. See Section \ref{minimal_polynomial}.

\section{Definition}

\begin{definition}[\cindex{diagonalizable}]
    A linear operator $T$ is diagonalizable if there is a basis $\beta$ of $V$ that $\coordinate{T}_\beta$ is a diagonal matrix. 
    
    If a linear operator $T$ is diagonalizable, it means for each $v_i \in \beta$, $T(v_i) = \lambda_i v_i$, . $v_i$ is called \cindex{eigenvector}\footnote{Eigenvector is also called \cindex{characteristic vector}.} and $\lambda$ is called \cindex{eigenvalue}\footnote{Eigenvalue is also called \cindex{characteristic value}.}. Here $v_i \neq 0$ because it is a basis\footnote{A eigenvalue could be $0$, but eigenvector could not be $\vec{0}$.}.
    
    An eigenvector is an invariant subspace of dimension $1$
    
    A square matrix $A$ is diagonalizable if $L_A$ is diagonalizable.
\end{definition} 



According to the definition, if a square matrix $A$ is diagonalizable, it is similar to a diagonal matrix. Let $Q$ be the matrix that contains this special basis $\beta$ (i.e., $\selectmatrixcolumn{Q}{j} = v_j$), and $\Lambda$ be the diagnoal matrix contains the corresponding eigenvalues (i.e., $\Lambda_{ii} = \lambda_i$). We have:
\begin{equation}
    A = \inverse{Q} \Lambda Q
\end{equation}




% calculation
\section{Calculation}

\begin{definition}\cindex{characteristic polynomial}
    It is easier to calculate eigenvalue using matrix representation. If (and only if) $\lambda$ is an eigenvalue of $A$, then 
\begin{equation}
    \determinate{A - \lambda I_n} = 0
\end{equation}

The function $f(t) = \determinate{A - t I_n}$ is called the characteristic polynomial of $A$. So our job is to find all zeros of $f(t)$.
\end{definition}



\begin{theorem}
    Every eigenvalue has at least one eigenvector.    
\end{theorem}


\begin{theorem}
    The choice of basis $\beta$ did not change the eigenvalue of $T$.
\end{theorem}

\begin{theorem}
Similar matrices have the same characteristic function.    
\end{theorem}

\begin{proof}
    Let $B = \inverse{P}AP$, we have 
    \begin{equation}
        \absolutevalue{B - \lambda I} = \absolutevalue{\inverse{P} A P - \lambda \inverse{P} I P} = \absolutevalue{\inverse{P} (A - \lambda I) P} = \absolutevalue{A - \lambda I}
    \end{equation}
\end{proof}


\begin{theorem}\label{eigenvector_sets_are_linearly_independent}
    Let $\lambda_k$ be distinct eigenvalues of $T$, and $S_i$ are linearly independent eigenvectors for $\lambda_i$. Then $\bigcup_{i=1}^k S_i$ are linearly independent\footnote{linearly independent is a weak property. If $T$ is self-adjoint, eigenvectors are all orthogonal, as proved in \theoref{normal_eigenvectors_are_orthogonal}.}.
\end{theorem}
\begin{proof}
    Assume they are not linearly independent and multiply the equation by $T - \lambda_k I$. Use deduction to prove.
\end{proof}

\theoref{eigenvector_sets_are_linearly_independent} shows that if $T$ has $n$ distinct eigenvalue, then $T$ must be diagonalizable.

\begin{definition}
    A polynomial $f(t)$ \cindex{split}s over $F$ if there are distinct scalars $\set{a_i}$ that
    \begin{equation}
        f(t) = c (t - a_1)^{k_1} (t - a_2)^{k_2} \hdots (t - a_j)^{k_j}
    \end{equation}
    
    The $F$ could be $\realnumber$ or $\complexnumber$. A function over $\complexnumber$ could always split. So we have to be careful when talking about the split over $F$.
    
    $k_j$ is called the \cindex{multiplicity} of $a_j$.
\end{definition}

If $T$ is diagonalizable, it must split. But split does not mean diagonalizable. The edge cases happen when some $a_i$ are the same.


\begin{theorem}
    Let $\lambda$ be an eigenvalue of $T$ with multiplicity $m$. Then $1 \leq \dimension{E_\lambda} \leq m$.
\end{theorem}
\begin{proof}
    Consider the basis $\set{v_p}$ for $E_\lambda$. Extend it to the ordered basis $\beta = \set{v_1, ..., v_p, v_{p+1}, ... v_n}$ of $V$, and let $A = \coordinate{T}_\beta$. $A$ now looks like:
    \begin{equation*}
        A = \begin{pmatrix}
            \lambda I_p & B \\
            0 & D
        \end{pmatrix}
    \end{equation*}
    
    So the characteristic polynomial of $A$ is:
    \begin{equation*}
        \begin{aligned}
            f(t) = \determinate{A - t I_n}  = \determinate{\begin{pmatrix}
                (\lambda - t) I_p & B \\
                0 & D - t I_{n-p}
            \end{pmatrix}} = \determinate{(\lambda - t) I_p} \times \determinate{D - t I_{n-p}}  = (\lambda - t)^p g(t)
        \end{aligned}
    \end{equation*}
    
    So $p= \dimension{E_\lambda} \leq m$ .
\end{proof}

The test for diagonalizability now requires $\dimension{E_\lambda} = m$ for each eigenvalue. So the procedure is:
\begin{enumerate}
    \item The characteristic polynomial of $T$ must split
    \item For each eigenvalue $\lambda$ of $T$, the multiplicity must equals the $\dimension{E_\lambda} = n - \rank{T - \lambda I_n}$)
    \item Select $\dimension{E_\lambda}$ vectors from the eigenspace of $\lambda$. They are the eigenvectors
    \item All eigenvectors of all eigenvalues forms a basis of $V$
    \item Create a matrix $Q$ whose columns are eigenvectors. Then $\inverse{Q} A Q$ is a diagonal matrix with diagonals as eigenvalues
\end{enumerate}

\begin{definition}[\cindex{direct sum}]
    Let $\set{W_i}$ be subspace of $V$. If $\displaystyle W_i \cap \sum_{j \neq i} W_j = \emptyset$ for all $i$, we define direct sum
    \begin{equation}
        W = \bigoplus_{j} W_j
    \end{equation}
\end{definition}

\begin{theorem}
    $T$ is diagonalizable if and only if $V$ is the direct sum of eigenspace of $T$.    
\end{theorem}



\begin{theorem}\label{split_means_upper_trangular}
    \emph{If the characteristic polynomial splits, $A$ is similar to an upper diagonal matrix}\footnote{This theorem is used many times in inner product.}.    
\end{theorem}
\begin{proof}
    If it splits, there must be an eigenvector $v_1$ for eigenvalue $\lambda_1$. Expand $v_1$ to a basis of $V$. Create a matrix $P$ whose columns are these basis ($v_1$ is the first column). Then:
    \begin{equation*}
        \inverse{P} A P = \begin{pmatrix}
            \lambda_1 & B \\
            0 & C
        \end{pmatrix}
    \end{equation*}
    
    $C$ is a $(n-1) \times (n-1)$ matrix and it splits (similar matrix has the same characteristic polynomial). So by induction there is $Q$ that $U= \inverse{Q} C Q$ is an upper diagonal matrix. Define $\realnumber$ as:
    \begin{equation*}
        R = \begin{pmatrix}
            1 & 0 \\
            0 & Q
        \end{pmatrix}
    \end{equation*}
    
    Now we calculate $\inverse{R} (\inverse{P} A P) R$:
    \begin{equation*}
        \begin{aligned}
            \inverse{R} (\inverse{P} A P) R  &= \begin{pmatrix}
                1 & 0 \\
                0 & \inverse{Q}
            \end{pmatrix} \begin{pmatrix}
                \lambda_1 & B \\
                0 & C
            \end{pmatrix} \begin{pmatrix}
                1 & 0 \\
                0 & Q
            \end{pmatrix} \\
            & = \begin{pmatrix}
                1 & 0 \\
                0 & \inverse{Q}
            \end{pmatrix} \begin{pmatrix}
                \lambda_1 & B Q \\
                0 & C Q
            \end{pmatrix} \\
            &= \begin{pmatrix}
                \lambda_1 & BQ \\
                0 & \inverse{Q} C Q
            \end{pmatrix}
        \end{aligned}     
    \end{equation*}
    
    Idea on how to choose the $Q$ : left multiplication will change the row, and right multiplication will change the column. So if we multiply left and right together, we will have $\inverse{Q} C Q$.
\end{proof}

\begin{theorem}
    For any $x_i$ in eigenspace $W_i$ with eigenvalue $\lambda_i$, we have $T(x_i) = \lambda_i x_i$. So each $W_i$ is an invariant subspace of $V$.
\end{theorem}

\begin{theorem}
    Let $\lambda_i$ be the eigenvalue of $A$ (with duplication), we have:
    \begin{equation}
        \trace{A} = \sum_i \lambda_i
    \end{equation}
    \begin{equation}
        \absolutevalue{A} = \prod_i \lambda_i
    \end{equation}
\end{theorem}
\begin{proof}
    The characteristic function of $A$ is $\absolutevalue{A - \lambda I} = 0$. Calculate what's before $\lambda$. Also take $\lambda=0$ and we have the proof for the second one.
\end{proof}


\begin{theorem}
    The rank of $A$ is the number of nonzero $\lambda_i$ (includes multiplicity). So $A$ is invertible if and only if $0$ is not an eigenvalue.    
\end{theorem}


\begin{theorem}
    The eigenvalues of $\inverse{T}$ are $\displaystyle \set{\frac{1}{\lambda_i}}$.
\end{theorem}



% invariant subspace
\section{Invariant Subspace}

Invariant subspace is very useful because it could split the big matrix representation into smaller ones. And it always exists.

\begin{definition}[\cindex{$T$-invariant subspace}]
    A subspace $W$ of $V$ is $T$-invariant subspace if $T(W) \subseteq W$.
\end{definition}

\begin{example}
    Let $p \in P_m(F)$ be a polynomial and $T \in \mathcal{L}(V)$. Then $\nullspace{p(T)}$ and $\rangespace{p(T)}$ are invariant under $T$.
\end{example}
\begin{proof}
    Suppose $u \in \nullspace{p(T)}$. Then $p(T)u = 0$. So $p(T) \circ (Tu) = T \circ p(T) \circ u = 0$.
    
    Suppose $u \in \rangespace{p(T)}$. Then $u = p(T)v$. So $Tu = Tp(T)v = p(T) Tv \in \rangespace{p(T)}$.
\end{proof}

\begin{example}
    Let $\beta = \set{v_i}$ be a basis of $V$. If $\coordinate{T}_\beta$ is a diagonal matrix, then $\spanset{v_1, \cdots, v_k}$ is invariant under $T$ for $1 \leq k \leq n$.
\end{example}



\begin{definition}[\cindex{$T$-cyclic subspace}]
    $W = \set{ x, T(x), T^2(x), \cdots}$ is the $T$-cyclic subspace of $V$ generated by $x$. It is the smallest $T$-invariant subspace that contains $x$.
    
    For $T$-cyclic subspace of a $n$ dimension space, there are at most $n$ elements in it. 
\end{definition}


\begin{theorem}\label{split_of_invariant_subspace}
    Let $W$ be $T$-invariant subspace of $V$. Then the characteristic polynomial of $\functionrestriction{T}{W}$ divides that of $T$.    
\end{theorem}
\begin{proof}
    Find a basis $\gamma$ of $T_W$ and expand it to basis $\beta$ of $V$. We have 
    \begin{equation*}
        \coordinate{T}_\beta = \begin{pmatrix}
            \coordinate{T}_\gamma & B \\
            0 & C        
        \end{pmatrix}
    \end{equation*}
    Then the characteristic polynomial of $\coordinate{T}_\beta$ could be expressed as the multiply of $\coordinate{T}_\gamma$.
\end{proof}

\begin{theorem}
    Let $V = \bigoplus W_i$ where each $W_i$ is a $T$-invariant subspace. Let $f_i$ be the characteristic polynomial of $T_{W_i}$, and $f$ is the characteristic polynomial of $T$. We have 
    \begin{equation}
        f(t) = \prod f_i (t)
    \end{equation}
\end{theorem}



\begin{theorem}\label{char_poly_of_invariant_subspace}
    Let $W$ be a $T$-cyclic subspace of $V$ generated by $v$, and let $k = \dimension{W}$. We have
    \begin{enumerate}
        \item $\set{v, T(v), T^2(v), ..., T^{k-1}(v)}$ is a basis of $W$\label{cyclic_first_conclusion}.
        \item If $a_0 v + a_1 T(v) + a_2 T^2(v) + ... + a_{k-1} T^{k-1} (v) + T^k (v) = 0$ (there must be one because they are not linearly independent), then the characteristic polynomial of $T_W$ is $f(t) = (-1)^k (a_0 + a_1 t + a_2 t^2 + a_{k-1} t^{k-1} + t^k)$
    \end{enumerate}
\end{theorem}
\begin{proof}
    Let $\beta$ be the basis of $W$ in conclusion~\ref{cyclic_first_conclusion}. Column $j$ of $\coordinate{T}_\beta$ is 
    \begin{equation*}
        T(\beta_j) = T\mleft(T^{j-1}(v)\mright) = T^{j}(v) = \begin{pmatrix}
            0 \\
            \vdots \\
            1_{j+1} \\
            \vdots \\
            0
        \end{pmatrix}
    \end{equation*}
    
    So we have 
    \begin{equation*}
        \coordinate{T}_\beta = \begin{pmatrix}
            0 & 0 & \hdots & 0 & -a_0 \\
            1 & 0 & \hdots & 0 & -a_1 \\
            0 & 1 & \hdots & 0 & -a_2 \\
            \vdots & \vdots & \ddots & \vdots & \vdots \\
            0 & 0 & \hdots & 1 & -a_{k-1}
        \end{pmatrix}
    \end{equation*}
    
    The characteristic polynomial of $\coordinate{T}_\beta$ happens to be $f(t) = (-1)^k (a_0 + a_1 t + a_2 t^2 + a_{k-1} t^{k-1} + t^k)$.
\end{proof}

\begin{theorem}[\cindex{Cayley-Hamilton}]
    Let $f$ be the characteristic polynomial of $T$. Then $f(T) = 0$.
\end{theorem}
\begin{proof}
    For any vector $v$, find its $T$-cyclic subspace $W$. Because of \theoref{char_poly_of_invariant_subspace}, there is a characteristic polynomial $f_W$ that $f_W(T) = 0$. Because of \theoref{split_of_invariant_subspace}, $f = f_W \times g$, so $f(T) = f_W(T) \times g(T) = 0$.
\end{proof}



% minimal polynomial
\section{Minimal Polynomial}\label{minimal_polynomial}

It is very hard to calculate the minimal polynomial of a linear transformation $T$. So it is mainly used as a proof.

\begin{theorem}[\cindex{factorization}]
    Suppose $p \in P_m(R)$ is a non-constant polynomial. Then $p$ has a unique factorization of the form
    \begin{equation}
        p(x) = c \mleft( \prod_{i=1}^m \mleft(x - \lambda_i\mright) \mright) \mleft( \prod_{j=1}^n \mleft(x^2 + b_j x + c_j\mright) \mright)
    \end{equation}
    Where $\lambda_i, b_j, c_j, c \in \realnumber$, and $b_j^2 < 4 c_j$ for all $j$.
\end{theorem}


\begin{theorem}
    Every operator $T$ on a finite-dimensional\footnote{finite-dimension is required} nonzero complex\footnote{complex space is required} vector space $V$ has an eigenvalue.
\end{theorem}
\begin{proof}
    Assume $\dimension{V} = n$. Choose $v \in V$ and $v \neq 0$. Then the set $\set{v, Tv, T^2v, \cdots, T^nv}$ are not linearly independent. So there is a $p \in P_n(\complexnumber)$ that $p(T) v = 0$. The trick is to choose $p$ with the \emph{minimal degree}.
    
    
    Since $p$ must have a zero in $\complexnumber$, we have $p(T) = (T - \lambda I) q(T)$. Since $p$ is the one with minimal degree, $q(T)v \neq 0$. So $p(T) v = (T - \lambda I) q(T) v = (T - \lambda I) v = 0$.
\end{proof}

\begin{definition}[\cindex{monic polynomial}]
    A monic polynomial is a polynomial whose highest degree coefficient is $1$.
\end{definition}

\begin{theorem}[\cindex{minimal polynomial}]
    Suppose $V$ is finite dimensional and $T \in \mathcal{L}(V)$. Then these exists a unique monic polynomial $p\in P(F)$ of smallest degree that $p(T) = 0$. And $\text{degree }p \leq \dimension{V}$. $p$ is called the minimal polynomial of $T$. 
\end{theorem}
\begin{proof}
    Deduct on the dimension of $V$. Assume $\dimension{V} = n$. For any $v \in V$ such that $v \neq 0$, the set $\set{v, Tv, T^2v, \cdots, T^n v}$ with $n+1$ element is linearly dependent. So there is smallest $m \leq n$ that $T^m v$ is a linear combination of $\set{v, Tv, T^2v, \cdots, T^{m-1} v}$. So there is polynomial $q$ with $\set{c_i}$ that
    \begin{equation*}
        q(T) v = c_0 v + c_1 Tv + c_2 T^2 v + \cdots + c_{m-1} T^{m-1} v + T^m v = 0
    \end{equation*}
    
    Since $q(T)T^k v = T^k q(T) v = 0$ for $0 \leq k \leq m-1$, $\dimension{\nullspace{q(T)}} \geq m$, so $\dimension{\rangespace{q(T)}} \leq n - m$. Use deduction, there is $s$ with the above property and degree $\leq n - m$ for $\functionrestriction{T}{\rangespace{q(T)}}$. So $sq \circ Tv = s(T) \circ q(T) v = 0$.
    
    Key idea: choose a random $v$ to find a polynomial $q$. Since the range dimension of $q$ is small, use deduction assumption to find a $s$ that $s \circ q(T)v =0$ for all $v$.
\end{proof}


\begin{theorem}
    Suppose $V$ is finite dimensional and $T \in \mathcal{L}(V)$. Then
    \begin{itemize}
        \item The zeros of the minimal polynomial of $T$ are the eigenvalues of $T$.
        \item If $V$ is a complex vector space, then the minimal polynomial of $T$ has the form $\displaystyle \prod_i \mleft(z-\lambda_i\mright)$ where $\set{\lambda_i}$ are the eigenvalues\footnote{With repetitions} of $T$.
        \item Every monic polynomial is the minimal polynomial of some operator.
    \end{itemize}
\end{theorem}
\begin{proof}
    If $\lambda \in F$ is a zero of $p$, we have $p(T)v = (T - \lambda I)q(T) v = 0$. Since $q$ is not the minimal polynomial, there is a $v$ that $u = q(T)v != 0$. So $(T - \lambda I) u = 0$.
    
    For any eigenvalue $\lambda$ of $T$, there is $v$ that $Tv = \lambda v$, and $T^k v = \lambda^k v$. So $p(T)v = p(\lambda)v = 0$. So $\lambda$ is a zero of $p$.
\end{proof}


\begin{theorem}\label{polynomial_multiply_of_minimal_polynomial}
    $q(T) = 0$ $\Leftrightarrow$ $q$ is a polynomial multiply of minimal polynomial of $T$.
\end{theorem}
\begin{proof}
    Assume $p$ is the minimal polynomial of $T$. So $q = ps + r$. So $0 = q(T) = p(T) s(T) + r(T) = r(T) \Rightarrow r = 0$.
\end{proof}


\begin{theorem}
    Let $U \subset V$ that is invariant under $T$. Then the minimal polynomial $q$ of $T$ in $V$ is a polynomial multiply of the minimal polynomial $p$ of $\functionrestriction{T}{U}$.
\end{theorem}
\begin{proof}
    Since $p(T)u=0$ for $u \in U$, so $p(\functionrestriction{T}{u}) = 0$. Since $q(\functionrestriction{T}{u}) = 0$, \theoref{polynomial_multiply_of_minimal_polynomial} means $q/p$.
\end{proof}

\begin{theorem}
    $T$ is not invertible if and only the constant term of minimal polynomial $p$ of $T$ is $0$.
\end{theorem}
\begin{proof}
    $T$ is not invertible $\Leftrightarrow$ $0$ is an eigenvalue of $T$ $\Leftrightarrow$ $0$ is a zero of $p$ $\Leftrightarrow$ the constant term of $p$ is $0$.
\end{proof}


\begin{theorem}
    Here is the relationship between minimal polynomial $m_T$ and characteristic polynomial $c_T$:
    \begin{itemize}
        \item $m_T$ divides $c_T$
        \item The degree of $c_T$ is $n$
        \item The degree of $m_T$ is no larger than $n$
        \item The root of $m_T$ are eigenvalues of $T$
        \item The multiplicity of any root of $m_T$ is no larger than that in $c_T$
    \end{itemize}
\end{theorem}
































