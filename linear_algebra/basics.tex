\chapter{Vector Space}

The logic of this section:
\begin{enumerate}
    \item Define a vector space
    \item Introduce linear combination
    \item Define span set
    \item Define generating set
    \item Define linearly dependent and linearly independent
    \item Define basis
    \item Prove that every vector space has a basis, for both finite and infinite space
\end{enumerate}

% basics
\section{Definitions}

\begin{definition}
    A \cindex{vector space} $V$ (and its subspace) is over a field $F$\footnote{A field is usually denoted as $F$.} such as $\realnumber$ or $\complexnumber$. It is closed in $+$ and $*$ operation.
\end{definition}


Important terms in vector space: transpose, symmetric matrix\footnote{The word matrix is Latin for womb which is the same root as matrimony. The idea is that a matrix is a receptacle for holding numbers.}, upper triangular, diagonal matrix, trace, skew symmetric.

After the vector space is defined, we now check the relation among vectors. The basic relation is \cindex{linear combination} $v = \sum_{i=1}^{n} a_i u_i$. \emph{$n$ must be finite}, and there is no restriction on $a_i \neq 0$. With linear combination, we could then define the \cindex{span} of a subset $S$ of a vector space.

If we set restriction on linear combination, we have \cindex{linearly dependent} relation among $\set{u_i}$ that $\sum_{i=1}^{n} a_i u_i = 0$ and not all $a_i$ are $0$. \cindex{linearly independent} means all $a_i = 0$. A linearly independent \cindex{generating set} is called \cindex{basis}.


A vector space is finite dimensional if it has a finite span set. Note that the dimension of $\realnumber^2$ is $2$, while the dimension of $\complexnumber$ is $1$.


\begin{definition}
	the \cindex{Kronecker delta}  $\delta_{ij}$ is defined as 
	\begin{equation}
		\delta_{ij} = \begin{cases}
			1 & \text{, if } i = j \\
			0 & \text{, if } i \neq j
 		\end{cases}
	\end{equation}
\end{definition}

\begin{definition}
	The $n\times n$ \cindex{identity matrix} \cindex{$I_n$} is defined as $\mleft(I_n \mright)_{ij} = \delta_{ij}$.
\end{definition}



\section{Replacement Theorem}
Now comes the most important theorem:
\begin{theorem}[\cindex{Replacement Theorem}]\label{replacement_theorem}
    if $G$ has $n$ vectors and they generate $V$, and $L$ is a set of $m$ linearly independent vectors, then $m \leq n$ and there is a subset $H \subseteq G$ with $n-m$ vectors that $L \cup H $ generate $V$.
    
\end{theorem}

Theorem~\ref{replacement_theorem} means every basis has the same number of vectors, which is the dimension of a vector space. One corollary is if a subspace has the same dimension as the vector space, they are the same.


\begin{theorem}
    \begin{equation}
        \dimension{A\cup B} = \dimension{A} + \dimension{B} - \dimension{A \cap B}
    \end{equation}    
\end{theorem}


\section{Lagrange Polynomial}

\begin{definition}
    The set of all polynomials over field $F$ with degree $\leq m$ is denoted as \cindex{$\mathcal{P}_m (F)$}. The list $1, x, x^2, \cdots, x^m$ is the stand basis of $\mathcal{P}_m (F)$. The dimension of $\mathcal{P}_m (F)$ is $m+1$.
\end{definition}

\begin{example}[\cindex{Lagrange polynomial}]
   Famous Lagrange polynomial for $g \in \mathcal{P}_n (F)$: if it passed $n+1$ points $g(c_i) = b_i$, then define $f_i$ as:
\begin{equation}
    f_i(x) = \prod_{k=0,k \neq i}^{n} \frac{x - c_k}{c_i - c_k}
\end{equation}

Then $\displaystyle g = \sum_{i=0}^{n} b_i \times f_i $. (note: we have $n+1$ $f_i$, so the dimension is $n+1$). 
\end{example}



\section{Basis Theorem}
\begin{theorem}
    every vector space has a basis.
\end{theorem}
\begin{proof}
    For infinite dimension vector space, we need more work to do. First define a maximal linearly independent subset that is both linearly independent and maximal (no other linearly independent set could contain it). Then we could prove that a basis must be a maximal linearly independent set. The question is now on how to create a maximal linearly independent set from a linearly independent set. Now we have to use Hausdorff Maximal Principle to select a chain of linearly independent sets and prove their union is contained in the chain, so there must exists a maximal set.
\end{proof}


From here we should know that the behavior of a vector is defined by its basis. So we could usually reduce the infinite space problem to a finite set of vectors.

















