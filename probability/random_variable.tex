\chapter{Random Variable}

\section{Events}

\begin{definition}[sample space]
    The set of all possible outcome of an experiment is defined as \cindex{sample space} $S$.
\end{definition}

\begin{definition}[event]
    Any subset $E$ of sample space $S$ is defined as \cindex{event}.
\end{definition}

It is conventional to designate $E \cup F$ as $EF$.


\begin{definition}[mutually exclusive]
    If $EF = \emptyset $, $E$ and $F$ are said to be \cindex{mutually exclusive}.
\end{definition}


\begin{definition}[sigma algebra]
    A collection of subsets of $S$ is called \cindex{sigma algebra} (or \cindex{Borel field}) $\mathcal{B}$ if
    \begin{enumerate}
        \item $\emptyset \in \mathcal{B}$
        \item If $A \in \mathcal{B}$, then $A^c \in \mathcal{B}$
        \item If $\set{A_i} \in \mathcal{B}$, then $\cup_{i=1}^\infty A_i \in \mathcal{B}$
    \end{enumerate}
\end{definition}


\begin{definition}[probability function]
    For a sample space $S$ and the associated sigma algebra $\mathcal{B}$, a \cindex{probability function} $P$ with domain $\mathcal{B}$ satisfies:
    \begin{enumerate}
        \item $P(A) \geq 0$ for all $A \in \mathcal{B}$
        \item $P(S) = 1$
        \item If $\set{A_i}$ are pairwise disjoint, then        
            \begin{equation}
                P \mleft(\bigcup_{n=1}^\infty E_n \mright) = \sum_{n=1}^\infty P(E_n)
            \end{equation}
    \end{enumerate}
    
    This definition is called the \cindex{axiom of probability}, or \cindex{Kolmogorov Axiom}. The countable additivity was rejected by deFinetti who choose the finite additivity.
\end{definition}


If the experiment is repeated over and over again, with probability $1$ the proportion of time that event $E$ will occur is $P(E)$.

\begin{theorem}[inclusion-exclusion identity]
    \begin{equation}
        \begin{aligned}
            P\mleft(\bigcup_{i=1}^n E_i \mright) &= \sum_i P(E_i) - \sum_{i < j}P(E_i E_j) + \sum_{i < j < k} P(E_i E_j E_k) + \cdots + (-1)^{n+1} P(E_1 E_2 \dots E_n)
        \end{aligned}
    \end{equation}
\end{theorem}

\begin{definition}[conditional probability]
    The \cindex{conditional probability} that $E$ occurs given that $F$ occurs is denoted by $P(E|F)$ and defined as:
    \begin{equation}
        P(E|F) = \frac{P(EF)}{P(F)}
    \end{equation}
    
    For conditional probability the sample space is changed.
\end{definition}

\begin{definition}[independent]
    Two event $E$ and $F$ are \cindex{independent} if $P(EF) = P(E) P(F)$.
\end{definition}

\begin{theorem}
    $E$ and $F$ are independent if $P(E|F) = P(E)$.    
\end{theorem}

\begin{definition}
    The events $\{ E_i \}$ are independent if for every subset $E_{i_j}$ ($ \forall j, 1 \leq j \leq n$), that
    \begin{equation}
        P(E_{i_1} E_{i_2} \dots E_{i_j}) = P(E_{i_1}) P(E_{i_2}) \cdots P(E_{i_j})
    \end{equation}
\end{definition}

\begin{definition}
    For mutually exclusive event $F_i$ that $\displaystyle \bigcup_{i=1}^n F_i = S$ .The \cindex{Bayes' rule} is defined as:
    \begin{equation}
    \displaystyle P(F_j | E) = \frac{P(EF_j)}{\sum\limits_{i=1}^n P(EF_i)} = \frac{P(E|F_j) P(F_j)}{\sum\limits_{i=1}^n P(E|F_i) P(F_i)}
    \end{equation}
\end{definition}

\begin{example}
    One classic example in traditional probability is to select $r$ balls from $n$ samples. We could do it with replacement or without replacement (\cindex{replacement} means put it back). In this experiment the sample space is finite and all outcomes are equally likely. The result is:

\begin{table}[H]
\centering
% \renewcommand*{\arraystretch}{1.4}
\begin{tabular}[t]{ccc}
\hline
 & Without replacement & With replacement \\
Ordered & $\displaystyle \frac{n!}{(n-r)!}$ & $n^r$ \\ 
Unordered & $\displaystyle {n \choose r}$ & $\displaystyle {{n + r -1} \choose r}$ \\
\hline
\end{tabular}
\end{table}
\end{example}
\begin{proof}
    For the unordered case with replacement, we want to solve the following formula:
    \begin{equation}
        x_1 + x_2 + \cdots + x_n = r \text{ (} x_i \geq 0 \text{)}
    \end{equation}
\end{proof}




% random variable


\section{Random Variable}

\begin{definition}[random variable]
    A real value function defined on sample space is called \cindex{random variable}.
\end{definition}

\begin{definition}[discrete random variable]
    A random variable that can takes at most a countable values is said to be \cindex{discrete random variable}.
\end{definition}

\begin{definition}[probability mass function]
    The \cindex{probability mass function} (pmf) $p(a)$ of discrete random variable $X$ is defined as $p(a) = \probability{X = a}$.
\end{definition}


\begin{definition}[cdf]
    The \cindex{cumulative distribution function} (cdf) of random variable $X$ for any $- \infty < b < \infty$ is defined as $F(b) = \probability{ X \leq b }$
\end{definition}

\begin{theorem}
    Some properties of cdf are:
    \begin{enumerate}
        \item $F(b)$ is nondecreasing function of $b$.
        \item $F(b)$ is continuous from the right.
        \item $\lim_{b \rightarrow \infty} F(b) = F(\infty) = 1$.
        \item $\lim_{b \rightarrow -\infty} F(b) = F(- \infty) = 0$.
    \end{enumerate}
\end{theorem}

\begin{definition}
    $\probability{X < b}$ is defined as $\probability{ X < b } = \displaystyle \lim_{h \rightarrow 0^+} \probability{ X \leq b - h } = \lim_{h \rightarrow 0^+} F(b-h)$.
\end{definition}

$\probability{X < b} \neq F(b)$ because $F(b)$ also include the probability that $X$ equals $b$.



\begin{definition}
    $f(x)$ is called \cindex{probability density function} (pdf) if $\probability{X \in B} = {\displaystyle \int_B f(x) \dif{x}} $.
\end{definition}

Because $\probability{ a - \frac{\varepsilon}{2} \leq X \leq a + \frac{\varepsilon}{2} } =\displaystyle \int_{a - \frac{\varepsilon}{2}}^{a + \frac{\varepsilon}{2}} f(x) \dif{x} \approx \varepsilon f(a)  $, $f(a)$ is a measure of how likely it is that the random variable will be near $a$ within interval $\varepsilon$.


Sometimes when calculating the probability $\probability{X=a}$, we can first calculate $F(a)$ and then calculate $\probability{X=a} = \dod{F(a)}{a}$.

\begin{definition}[expectation]
    The \cindex{expectation} of $X$ is defined as:
    \begin{equation}
        \expect{X} = \begin{cases}
            \displaystyle \sum x p(x) & \text{for discrete case} \\
            \displaystyle \int_{-\infty}^{\infty} x f(x) \dif{x} & \text{for continuous case}
        \end{cases}
    \end{equation}
\end{definition}




\begin{theorem}
    The expectation of a function $g$  of a random variable $X$ is:
    \begin{equation}
        \expect{g(X)} = \begin{cases}
            \displaystyle \sum g(x) p(x) & \text{for discrete case} \\
            \displaystyle \int_{-\infty}^{\infty} g(x) f(x) \dif{x} & \text{for continuous case}
        \end{cases}
    \end{equation}
\end{theorem}

\begin{theorem}
    \begin{equation}
        \expect{aX + bY} = a \expect{X} + b \expect{Y}
    \end{equation}    
\end{theorem}

\begin{definition}[variance]
    The \cindex{variance} of $X$ is defined as
    \begin{equation}
        \begin{aligned}
            \variance{X} &= \expect{(X - \expect{X})^2} \\
            &= \expect{X^2} - \expect{X}^2
        \end{aligned}
    \end{equation}
\end{definition}


\begin{theorem}
    Let $X_i$ be identically independent random variable. We need to notice that 
    \begin{equation*}
        \variance{\sum_{i=1}^n X_i} = n \variance{X} \neq \variance{n \times X_i} = n^2 \variance{X}
    \end{equation*}    
\end{theorem}



% joint distribution
\section{Joint Distribution}

\begin{definition}
    The \cindex{joint cumulative probability distribution function} of two discrete random variable $X$ and $Y$ is defined as
    \begin{equation}
        F(a,b) = \probability{ X \leq a, Y \leq b }
    \end{equation}
    
    The cumulative distribution of $Y$ is defined as
    \begin{equation}
        F_X(a) = \probability{X \leq a } =\probability{X \leq a, Y < \infty } = F(a, \infty)
    \end{equation}
    
    The \cindex{joint probability mass function} is defined as 
    \begin{equation}
        p(x,y) =\probability{X = x, Y = y}
    \end{equation}
    
    The probability mass function of $X$ from $p(x,y)$ is defined as
    \begin{equation}
        p_X(x) = \displaystyle \sum_{y: p(x,y) > 0} p(x,y)
    \end{equation}
\end{definition}


\begin{definition}
    for continuous case, $X$ and $Y$ are \cindex{jointly continuous} if there is a function $f(x,y)$ that for all sets $A$ and $B$ we have 
\begin{equation}
    \probability{X \in A, Y \in B} = \int_B \int_A f(x,y) \dif{x} \dif{y}
\end{equation}

$f(x,y)$ is called \cindex{joint probability density function} of $X$ and $Y$. The probability density function of $X$ can be obtained as
\begin{equation}
    \begin{aligned}
        \probability{X \in A} &= \int_A \mleft( \int_{-\infty}^\infty f(x,y) \dif{y} \mright) \dif{x}  \\
        &= \int_A f_X (x) \dif{x}
    \end{aligned}
\end{equation}
where
\begin{equation}
     f_X(x) = \int_{-\infty}^\infty f(x,y) \dif{y}
\end{equation}

We have 
\begin{equation}
    \dmd{F(a,b)}{2}{a}{1}{b}{1} = f(a,b)
\end{equation}
\end{definition}

For $n$ random variable $X_i$, we could construct $Y_i$ that
\begin{equation}\label{jointdenityequation}
    \begin{aligned}
        Y_1 &= g_1 (X_1, X_2, \cdots, X_n) \\
        Y_2 &= g_2 (X_1, X_2, \cdots, X_n) \\
        & \vdots \\
        Y_n &= g_n (X_1, X_2, \cdots, X_n) \\
    \end{aligned}
\end{equation}

Assume its \cindex{Jacobian} determinant $J(x_1, \dots, x_n) \neq 0$ for all $x_i$. Then the probability density function of $Y_i$ is 
\begin{equation}
    f_{Y_1, \cdots, Y_n}(y_1, \cdots, y_n) = \frac{1}{\absolutevalue{J(x_1, \cdots, x_n)}} f_{X_1, \cdots, X_n}(x_1, \cdots , x_n)
\end{equation}

So the process of calculating $f_{Y_i}$ are:
\begin{enumerate}
    \item solve $X_i = h(Y_i)$ from equation (\ref{jointdenityequation} ).
    \item calculate $J(X_i)$.
    \item replace $x_i$ by $y_i$ in $f(x_i)$ and multiply by $\displaystyle \frac{1}{\absolutevalue{J}}$.
\end{enumerate}

\begin{theorem}
    Let $X$ has continuous cdf $F(X)$ and define a random variable $Y = F(X)$. Then $Y$ is a uniform distribution on $(0,1)$.
    
    The implication is, if we want to generate random samples with distribution $F(X)$, we can choose a uniform random variable on $(0,1)$ and generate a sample $y$, and calculate $x = \inverse{F}(y)$.
\end{theorem}
\begin{proof}
    Calculate the cdf of $Y$:
    \begin{equation}
        \begin{aligned}
            \probability{Y \leq y} &= \probability{F(X) \leq y} \\
            &= \probability{ \inverse{F} \mleft(F(X)\mright) \leq \inverse{F} (y) }\\
            &= \probability{X \leq \inverse{F} (y) } \\
            &= F\mleft(\inverse{F} (y) \mright) \\
            &= y
        \end{aligned}
    \end{equation}
\end{proof}



% independence

\section{Independence}

\begin{definition}
    Two random variable $X$ and $Y$ are \cindex{independent} if $\forall a,b \in \mathbf{R}$,
    \begin{equation}
        \probability{ X \leq a , Y \leq b } = \probability{ X \leq a } \probability{ X \leq b }
    \end{equation}
    It means
    \begin{equation}
        F(a,b) = F_X(a) F_Y(b)
    \end{equation}
    When $X$ and $Y$ are discrete, it reduces to
    \begin{equation}
        p(x,y) = p_X(x) p_Y(y)
    \end{equation}
    
    When they are continuous, it reduces to
    \begin{equation}
        f(x,y)=f_X(x) f_Y(y)
    \end{equation}
\end{definition}

\begin{theorem}
    If $X$ and $Y$ are indepent, for any function $h$ and $g$, we have
    \begin{equation}
        \expect{g(X) h(Y)} = \expect{g(X)} \expect{h(Y)}
    \end{equation}    
\end{theorem}

\section{Covariance}

\begin{definition}[covariance]
    The \cindex{covariance} for $X$ and $Y$ is defined as 
    \begin{equation}
    \begin{aligned}
        \covariance{X}{Y} &= \expect{(X - \expect{X})(Y - \expect{Y})} \\
        &= \expect{XY} - \expect{X} \expect{Y}
    \end{aligned}
    \end{equation}
    In general covariance means $Y$ tends to move in the same direction of $X$.
\end{definition}

\begin{theorem}
    \begin{equation}
        \begin{aligned}
            \covariance{X}{X} &= \variance{X} \\
            \covariance{X}{Y} &= \covariance{Y}{X} \\
            \covariance{cX}{Y} &= c \covariance{X}{Y} \\
            \covariance{X}{Y + Z} &= \covariance{X}{Y} + \covariance{X}{Z} \\
            \mathhilight{Cov} \mleft(\sum_{i=1}^n X_i, \sum_{j=1}^m Y_j \mright) &= \sum_{i=1}^n \sum_{j=1}^m \mathhilight{Cov}(X_i, Y_j) \\
        \end{aligned}
    \end{equation}    
\end{theorem}

\begin{theorem}
    \begin{equation}
    \begin{aligned}
        \mathhilight{Var} \mleft( \sum_{i=1}^n X_i \mright) &= \mathhilight{Cov}\mleft( \sum_{i=1}^n X_i, \sum_{j=1}^n X_j \mright) \\
        &= \sum_{i=1}^n \variance{X_i} + 2 \sum_{i=1}^n \sum_{j < i} \covariance{X_i}{X_j} \\
        &= \sum_{i=1}^n \variance{X_i} \text{ (if } X_i \text{,} X_j \text{ are independent)}
    \end{aligned}
    \end{equation}
    This theorem is often used to calculate the variance.
\end{theorem}

\begin{definition}
    Function $F_{X+Y}$ is called the \cindex{convolution} of the distribution of $F_X$ and $F_Y$, which is
    \begin{equation}
        \begin{aligned}
            F_{X+Y}(a) &= \probability{X + Y \leq a} \\
            &= \iint_{x+y \leq a} f(x) g(y) \dif{x} \dif{y} \\
            &= \int_{-\infty}^\infty \mleft( \int_{-\infty}^{a - y} f(x) \dif{x} \mright) g(y) \dif{y} \\
            &= \int_{-\infty}^\infty F_X (a-y)g(y) \dif{y}
        \end{aligned}
    \end{equation}
    
    The probability density function $f_{X+Y}(a)$ is given by 
    \begin{equation}
        \begin{aligned}
            f_{X+Y}(a) &= \dod{F_{X+Y}(a)}{a} \\
            &= \dod{\displaystyle \int_{-\infty}^\infty F_X (a-y)g(y) \dif{y}}{a} \\
            &= \int_{-\infty}^{\infty} f(a-y) g(y) \dif{y}
        \end{aligned}
    \end{equation}
\end{definition}



% moment generating function

\section{Moment Generating Function}

\begin{definition}
    The \cindex{moment generating function} $\phi_X (t)$ of the random variable $X$ is defined by
    \begin{equation}
        \begin{aligned}
            \phi_X (t) &= \expect{e^{tX}} \\
            &= \begin{cases}
                 \displaystyle \sum_x e^{tx} p(x)\\
                 \displaystyle \int_{-\infty}^\infty e^{tx} f(x) \dif{x}
            \end{cases}
        \end{aligned}
    \end{equation}
    
    For any $n$ random variable $X_i$, the \cindex{joint moment generating function} $\phi(t_i)$ is defined by $\phi(t_1, \dots, t_n) = \expect{ e^{ \sum_{i=1}^n t_i X_i}}$.
\end{definition}

Because $\phi^n (t) = \expect{X^n e^{tX}}$, we have $\phi^n(0) = \expect{X^n}$.

\begin{theorem}
    For random variable $X + Y$, the moment generation function is
    \begin{equation}
        \phi_{X+Y}(t) = \phi_X(t) \phi_Y(t)
    \end{equation}    
\end{theorem}

\begin{theorem}
The moment generating function uniquely determine the distribution.    
\end{theorem}



% sample mean and sample variance

\section{Sample Mean and Sample Variance}


\begin{definition}[sample mean]
    If $X_i$ are independent and identically distributed, then the random variable $\bar{X} =\displaystyle \frac{\sum_{i=1}^n X_i}{n}$ is called the \cindex{sample mean}.
\end{definition}

\begin{definition}[sample variance]
    Let $X_i$ be independent and identically distributed random variable with mean $\mu$ and variance $\sigma^2$, the \cindex{sample variance} $S^2$ is defined by 
\begin{equation}
    S^2 = \sum_{i=1}^n \displaystyle \frac{(X_i - \bar{X})^2}{n - 1}
\end{equation} 
\end{definition}



\begin{theorem}
    Some commonly used identity for sample mean and sample variance:
    \begin{equation}
        \begin{aligned}
            \expect{\bar{X}} &= \mu \\
            \variance{\bar{X}} &= \frac{\sigma^2}{n} \\
            \expect{S^2} &= \sigma^2
        \end{aligned}
    \end{equation}
\end{theorem}
    
\begin{theorem}
\begin{equation}
    \covariance{\bar{X}}{X_i - \bar{X}} = 0
\end{equation}    
\end{theorem}



\begin{theorem}
If $X_i$ are independent and identically distributed normal random variable $\normaldistribution{\mu}{\sigma^2}$, then
    \begin{enumerate}
        \item the sample mean $\bar{X}$ and sample variance $S^2$ are independent.
        \item $\bar{X} \sim \normaldistribution{\mu}{\frac{\sigma^2}{n}}$
        \item $\displaystyle \frac{(n-1)S^2}{\sigma^2} \sim \chisquaredistribution{n-1}$
    \end{enumerate}   
\end{theorem}

\begin{proof}
    Because $X_1$ are normal random varialbe, $\bar{X}$ is also a normal random variable. Since $\covariance{\bar{X}}{X_i - \bar{X}} = 0$, normal random variable $\bar{X}$ and $X_i - \bar{X}$ are independent. Since $S^2 = \displaystyle \sum_{i=1}^n \frac{(X_i - \bar{X})^2}{n -1}$ is a function of $X_i - \bar{X}$ which is independ from $\bar{X}$, $S^2$ is independent from $\bar{X}$.
    
    Because 
    \begin{equation*}
        \frac{(n-1)S^2}{\sigma^2} + \mleft(\frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}}\mright)^2 = \sum_{i=1}^n \frac{(X_i - \mu)^2}{\sigma^2}
    \end{equation*}
    generate the moment generating function of $\displaystyle \frac{(n-1)S^2}{\sigma^2}$ from above formula and we have $\expect{\exponential{t \frac{(n-1)S^2}{\sigma^2}}} = \mleft(1-2t \mright)^{- \frac{n-1}{2}}$ which is an chi-squared distribution with freedom $n-1$.
\end{proof}


% inequality

\section{Inequality}

\begin{theorem}[\cindex{Markov's Inequality}]
If $X$ is a random variable that is non-negative. Then for any nonnegative function $g(x)$ and any $a > 0$ we have 
\begin{equation}
    \probability{g(X) \geq a} \leq \frac{\expect{g(X)}}{a}
\end{equation}    
\end{theorem}
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            \expect{g(X)} &= \int_0^\infty g(x) f(x) \dif{x} \\
            & \geq \int_{\set{x: g(x) \geq a}}^\infty g(x) f(x) \dif{x} \\
            & \geq a \int_{\set{x: g(x) \geq a}}^\infty f(x) \dif{x} \\
            &= a \probability{g(X) \geq a}
        \end{aligned}
    \end{equation*}
\end{proof}

\begin{theorem}[\cindex{Chebyshev's Inequality}]
    If $X$ is a random variable with mean $\mu$ and variance $\sigma^2$. Then for any $k > 0$ we have
    \begin{equation}
        \probability{\absolutevalue{X - \mu} \geq k } \leq \frac{\sigma^2}{k^2}
    \end{equation}
\end{theorem}
\begin{proof}
    Set $g(X) = (X - \mu)^2$ in Markov's Inequality.
\end{proof}

\begin{theorem}[\cindex{Strong Law of Large Numbers}]
    Let $X_i$ be a sequence of independent random variables having a common distribution. With probability 1:
    \begin{equation}
        \frac{X_1 + X_2 + \dots + X_n}{n} \rightarrow \mu \text{ as } n \rightarrow \infty
    \end{equation}
\end{theorem}

\begin{theorem}[\cindex{Central Limit Theorem}]
    Let $X_i$ be a sequence of independent identical random variables with mean $\mu$ and variance $\sigma^2$, the distribution $\displaystyle \frac{X_1 + X_2 + \dots + X_n - n \mu}{\sigma \sqrt{n}}$ tends to be $\normaldistribution{0}{1}$ as $n \rightarrow \infty$.
\end{theorem}






