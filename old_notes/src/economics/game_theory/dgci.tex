\section{Dynamic Games of Complete Information}

\subsection{Extensive Games}

\begin{definition}[\cindex{Nature's Choice}]
    The Nature will choose an action according to a probability distribution. The uncertainty is called \cindex{exogenous uncertainty}.
\end{definition}

\begin{definition}[\cindex{extensive game}]
    An extensive game with perfect information has the following components:
    \begin{enumerate}
        \item A set of $N$ players.
        \item A set $H$ of histories, which is a finite or infinite sequence of actions $a$ that satisfies the following 3 properties:
            \begin{enumerate}
                \item $\emptyset \in H$.
                \item Sub-history exists: if $(a^k)_{k \leq K} \in H$, and $L < K$, then $(a^k)_{k \leq L} \in H$.
                \item Infinite history exists: For an infinite sequence $(a^k)_{k < \infty}$, if $(a^k)_{k \leq L} \in H$ for all $L$, then $(a^k)_{k < \infty} \in H$.
            \end{enumerate}
            A history $h \in H$ is \cindex{terminal} if:
            \begin{enumerate}
                \item it is infinite,
                \item or there is no $a$ that $(h, a) \in H$.
            \end{enumerate}
            The set of all \cindex{nonterminal} history is $Z$.
        \item A \cindex{player function} $P: H\backslash Z \rightarrow N$ that assign each nonterminal history a member of $N$.
        \item The next actions is function $A$ that $A(h) = \set{a: (h, a) \in H}$.
        \item For each player $i$ a preference relation $\succsim_i$ on $Z$.
    \end{enumerate}
    The extensive game is denoted as $\Gamma = \left\langle N, H, P, \succsim_i \right\rangle$. It could be displayed as a tree called \cindex{game tree}.
\end{definition}

\begin{definition}[\cindex{strategy}]
    A strategy of player $i$ is a function $s_i: \set{h: P(h) = i} \rightarrow A(h)$. So it assigns an action in $A(h)$ for each nonterminal history $h$ that $P(h) = i$.
\end{definition}

\begin{definition}[\cindex{outcome}]
    The outcome $O(s)$ is the terminal history if each player $i$ follows his strategy $s_i$.
\end{definition}

The Nash equilibrium definition is the same for the extensive game.

\begin{definition}
    The strategic form $\left\langle N, S_i, \succsim_i^{'} \right\rangle$ of the extensive game with perfect information $\Gamma = \left\langle N, H, P, \succsim_i \right\rangle$ is that:
    \begin{enumerate}
        \item $S_i$ is the set of strategies of player $i$ in $\Gamma$.
        \item $\succsim_i^{'}$ is defined as $h_a \succsim_i^{'} h_b$ if and only if $O(h_a) \succsim_i O(h_b)$.
    \end{enumerate}
\end{definition}

\begin{definition}[\cindex{reduced strategy}]
    All histories will be classified into domains. A domain contains all histories that are linearly ordered by $\subset$. So a history may appear in multiple domains. A reduced strategy for player $i$ is the strategy for a domain. 
\end{definition}

\begin{definition}[\cindex{subgame}]
    For the game $\Gamma = \left\langle N, H, P, \succsim_i \right\rangle$, the subgame that follows the history $h$ is the game $\Gamma(h) = \left\langle N, H|_h, P|_h, \succsim_i|_h \right\rangle$ that:
    \begin{enumerate}
        \item $H|_h = \set{h': (h, h') \in H}$.
        \item $P|_h(h') = P(h, h')$ for each $h' \in H|_h$.
        \item $h' \succsim_i|_h h''$ if and only if $(h, h') \succsim_i (h, h'')$.
    \end{enumerate}
    
    For subgame, we define strategy and outcome as:
    \begin{enumerate}
        \item $s_i |_h(h') = s_i (h,h')$ for all $h' \in H|_h$.
        \item $O_h$ is the outcome function of $\Gamma(h)$.
    \end{enumerate}
\end{definition}

\begin{definition}[\cindex{subgame perfect equilibrium}]
    A subgame perfect equilibrium is a strategy profile $s^*$ that $s^*|_h$ is the Nash equilibrium for $\Gamma(h)$.
\end{definition}

\begin{theorem}[\cindex{one deviation property}]\label{gt:odp}
    A strategy profile $s^*$ satisfies one derivation property if for all $h\in H$ and $P(h) =i$, we have 
    \begin{equation*}
        O_h(s_i^*|_h, s_{-i}^* |_h)\ {\succsim_i}|_h\ O_h(s_i^{'}|_h, s_{-i}^* |_h)
    \end{equation*}
    where $s_i^{'}$ is a strategy of player $i$ that $s_i^{'}$ and $s_i^* |_h$ only differers at history $h$. Then $s^*$ is a subgame perfect equilibrium if it satisfies one deviation property.
\end{theorem}
\begin{proof}
    Assume the reverse. There is a history $h$ and player $k$ that $s_k^* |_h$ is not the best response to $s_{-k}^* |_h$ in $\Gamma(h)$. Find $\hat{s_k} \succ s_k^*$ so the size of $\set{(h,h') \in H: s_k^* |_h (h') \neq \hat{s_k}(h')}$ is minimal. Then find the longest history $\hat{h}$ that $s_k^* |_h (\hat{h}) \neq \hat{s_k}(\hat{h})$. $(\hat{s_k}, s_{-k}^* |_h)$ must pass through $\hat{h}$. Or $\hat{h}$ does not contribute to the outcome and we could let $s_k^* |_h (\hat{h}) = \hat{s_k}(\hat{h})$ and nothing is changed, which violate the assumption of $\hat{s_k}$. Now consider $\Gamma(\hat{h})$. $s_k^* |_h (\hat{h})$ and $\hat{s_k}(\hat{h})$ only differs in $\hat{h}$ and $s_k^* |_h (\hat{h}) \succ \hat{s_k}(\hat{h})$, a violation.
\end{proof}

\begin{theorem}[\cindex{Kuhn's theorem}]
    Every finite extensive game with perfect information has a subgame perfect equilibrium.
\end{theorem}
\begin{proof}
    Define $\absolutevalue{\Gamma}$ to be the length of the longest history in $\Gamma$. If $\absolutevalue{\Gamma(h)} = 0$, so $h$ is a terminal history, define $R(h) = h$. Assume $R(h)$ is defined for $\set{h \in H: \absolutevalue{\Gamma(h)} \leq K}$. Let $h^*$ be a history that $\absolutevalue{\Gamma(h^*)} = K+1$. The next player is $P(h^*) = i$. Define $s_i(h^*)$ to choose the best action from $A(h^*)$. Now $R(h^*) = R(h^*, s_i(h^*))$. Now we have defined $s$ and it is subgame perfect equilibrium according to Theorem~\ref{gt:odp}. The process is called \cindex{backwards induction}.
\end{proof}










\subsection{Other Definitions}

\begin{definition}
    Player $i$ has a collection of information $h_i$ with the following properties:
    \begin{enumerate}
        \item If $\absolutevalue{h_i} = 1$, player $i$ who moves at $x$ knows he is at $x$.
        \item If $\absolutevalue{h_i} \geq 2$, player $i$ who moves at $x$ does not know where he is.
        \item If $\absolutevalue{h_i} \geq 2$, let $x \in h_i$ and $y \in h_i$, then $A_i(x_i) = A_i(y_i)$. So the actions are indistinguishable, or player $i$ would know where he is.
    \end{enumerate}
    
    If $\absolutevalue{h_i} \geq 2$, we use ellipse to include all nodes within the same information set. We could also use dashed lines to connect them.
\end{definition}

\begin{definition}
    A game in which every information set is a singleton and there are no moves of Nature is called a \cindex{game of perfect information}. Otherwise, it is called a \cindex{game of imperfect information}.
\end{definition}

\begin{theorem}
    Any simultaneous-move game is a game of imperfect information.    
\end{theorem}

\begin{definition}[\cindex{pure strategy}]
    Let $A_i(h_i)$ be the set of actions a player $i$ can take at $h_i$, and let $A_i$ be the set of all actions of player $i$. A pure strategy for player $i$ is a function $s_i: H_i \rightarrow A_i$ that assigns an action $s_i(h_i) \in A_i(h_i)$ for all information set $h_i$. We denote by $S_i$ the set of all pure strategies.
\end{definition}

\begin{definition}[\cindex{mixed strategy}]
    A mixed strategy for player $i$ is a probability distribution over his pure strategy $s_i \in S_i$.
\end{definition}

\begin{definition}[\cindex{behavioral strategy}]
    A behavior strategy is $\sigma_i: H_i \rightarrow \bigtriangleup A_i(h_i)$ where $\sigma_i (a_i (h_i))$ is the probability that player $i$ plays action $a_i (h_i) \in A_i(h_i)$  in informaiton set $h_i$.
\end{definition}

Comparison between mixed strategy and behavioral strategy:
\begin{itemize}
    \item In mixed strategy, we choose a pure strategy for all possible $h_i$ before the game is played, and follow this pure strategy
    \item In behavior strategy, the randomness is chosen as the game unfolds. So for each $h_i$ we will randomly choose.
\end{itemize}

\begin{definition}[\cindex{perfect recall}]
    A game of perfect recall is one in which no player ever forgets information that he previously knew.
\end{definition}

\begin{theorem}
    Mixed and behavioral strategies are equivalent in a perfect recall game.    
\end{theorem}

\begin{definition}
    Let $\sigma^*$ be a Nash equilibrium profile of behavioral strategies in an extensive-form game. An information set is \cindex{on the equilibrium path} if given $\sigma^*$ it is reached with positive probability. Or it is called \cindex{off the equilibrium path}.
\end{definition}






































